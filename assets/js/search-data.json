{
  
    
        "post0": {
            "title": "A fastai Learner from Scratch",
            "content": "This final chapter (other than the conclusion and the online chapters) is going to look a bit different. It contains far more code and far less prose than the previous chapters. We will introduce new Python keywords and libraries without discussing them. This chapter is meant to be the start of a significant research project for you. You see, we are going to implement many of the key pieces of the fastai and PyTorch APIs from scratch, building on nothing other than the components that we developed in &lt;&gt;! The key goal here is to end up with your own Learner class, and some callbacks—enough to be able to train a model on Imagenette, including examples of each of the key techniques we&#39;ve studied. On the way to building Learner, we will create our own version of Module, Parameter, and parallel DataLoader so you have a very good idea of what those PyTorch classes do.&lt;/p&gt; The end-of-chapter questionnaire is particularly important for this chapter. This is where we will be pointing you in the many interesting directions that you could take, using this chapter as your starting point. We suggest that you follow along with this chapter on your computer, and do lots of experiments, web searches, and whatever else you need to understand what&#39;s going on. You&#39;ve built up the skills and expertise to do this in the rest of this book, so we think you are going to do great! . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Let&#39;s begin by gathering (manually) some data. . Data . Have a look at the source to untar_data to see how it works. We&#39;ll use it here to access the 160-pixel version of Imagenette for use in this chapter: . untar_data?? . Signature: untar_data( url: &#39;str&#39;, archive: &#39;Path&#39; = None, data: &#39;Path&#39; = None, c_key: &#39;str&#39; = &#39;data&#39;, force_download: &#39;bool&#39; = False, base: &#39;str&#39; = &#39;~/.fastai&#39;, ) -&gt; &#39;Path&#39; Source: def untar_data( url:str, # File to download archive:Path=None, # Optional override for `Config`&#39;s `archive` key data:Path=None, # Optional override for `Config`&#39;s `data` key c_key:str=&#39;data&#39;, # Key in `Config` where to extract file force_download:bool=False, # Setting to `True` will overwrite any existing copy of data base:str=&#39;~/.fastai&#39; # Directory containing config file and base of relative paths ) -&gt; Path: # Path to extracted file(s) &#34;Download `url` using `FastDownload.get`&#34; d = FastDownload(fastai_cfg(), module=fastai.data, archive=archive, data=data, base=base) return d.get(url, force=force_download, extract_key=c_key) File: ~/mambaforge/lib/python3.9/site-packages/fastai/data/external.py Type: function . FastDownload? . Object `FastDownload` not found. . Where does &#39;FastDownload&#39; come from??? . !ls -d /root/mambaforge/lib/python3.9/site-packages/fast* | egrep -v &#39; -info&#39; . /root/mambaforge/lib/python3.9/site-packages/fastai /root/mambaforge/lib/python3.9/site-packages/fastbook /root/mambaforge/lib/python3.9/site-packages/fastcore /root/mambaforge/lib/python3.9/site-packages/fastdownload /root/mambaforge/lib/python3.9/site-packages/fastjsonschema /root/mambaforge/lib/python3.9/site-packages/fastprogress /root/mambaforge/lib/python3.9/site-packages/fastrelease . As seen above, fastai or Jeremy has many packages installed with prefixed &#39;fast&#39;. How should we look into such source code??? . import fastdownload fastdownload.FastDownload?? . Init signature: fastdownload.FastDownload( cfg=None, base=&#39;~/.fastdownload&#39;, archive=None, data=None, module=None, ) Docstring: &lt;no docstring&gt; Source: class FastDownload: def __init__(self, cfg=None, base=&#39;~/.fastdownload&#39;, archive=None, data=None, module=None): base = Path(base).expanduser().absolute() default = {&#39;data&#39;:(data or &#39;data&#39;), &#39;archive&#39;:(archive or &#39;archive&#39;)} self.cfg = Config(base, &#39;config.ini&#39;, create=default) if cfg is None else cfg self.module = checks_module(module) if data is not None: self.cfg[&#39;data&#39;] = data if archive is not None: self.cfg[&#39;archive&#39;] = archive def arch_path(self): &#34;Path to archives&#34; return self.cfg.path(&#39;archive&#39;) def data_path(self, extract_key=&#39;data&#39;, arch=None): &#34;Path to extracted data&#34; path = self.cfg.path(extract_key) return path if arch is None else path/remove_suffix(arch.stem, &#39;.tar&#39;) def check(self, url, fpath): &#34;Check whether size and hash of `fpath` matches stored data for `url` or data is missing&#34; checks = read_checks(self.module).get(url) return not checks or path_stats(fpath)==checks def download(self, url, force=False): &#34;Download `url` to archive path, unless exists and `self.check` fails and not `force`&#34; self.arch_path().mkdir(exist_ok=True, parents=True) return download_and_check(url, urldest(url, self.arch_path()), self.module, force) def rm(self, url, rm_arch=True, rm_data=True, extract_key=&#39;data&#39;): &#34;Delete downloaded archive and extracted data for `url`&#34; arch = urldest(url, self.arch_path()) if rm_arch: arch.delete() if rm_data: self.data_path(extract_key, arch).delete() def update(self, url): &#34;Store the hash and size in `download_checks.py`&#34; update_checks(urldest(url, self.arch_path()), url, self.module) def extract(self, url, extract_key=&#39;data&#39;, force=False): &#34;Extract archive already downloaded from `url`, overwriting existing if `force`&#34; arch = urldest(url, self.arch_path()) if not arch.exists(): raise Exception(f&#39;{arch} does not exist&#39;) dest = self.data_path(extract_key) dest.mkdir(exist_ok=True, parents=True) return untar_dir(arch, dest, rename=True, overwrite=force) def get(self, url, extract_key=&#39;data&#39;, force=False): &#34;Download and extract `url`, overwriting existing if `force`&#34; if not force: data = self.data_path(extract_key, urldest(url, self.arch_path())) if data.exists(): return data self.download(url, force=force) return self.extract(url, extract_key=extract_key, force=force) File: ~/mambaforge/lib/python3.9/site-packages/fastdownload/core.py Type: type Subclasses: . It seems that you need to &#39;import&#39; here too to access the source of the indirectly called functions/classes. . URLs?? . Init signature: URLs() Source: class URLs(): &#34;Global constants for dataset and model URLs.&#34; LOCAL_PATH = Path.cwd() MDL = &#39;http://files.fast.ai/models/&#39; GOOGLE = &#39;https://storage.googleapis.com/&#39; S3 = &#39;https://s3.amazonaws.com/fast-ai-&#39; URL = f&#39;{S3}sample/&#39; S3_IMAGE = f&#39;{S3}imageclas/&#39; S3_IMAGELOC = f&#39;{S3}imagelocal/&#39; S3_AUDI = f&#39;{S3}audio/&#39; S3_NLP = f&#39;{S3}nlp/&#39; S3_COCO = f&#39;{S3}coco/&#39; S3_MODEL = f&#39;{S3}modelzoo/&#39; # main datasets ADULT_SAMPLE = f&#39;{URL}adult_sample.tgz&#39; BIWI_SAMPLE = f&#39;{URL}biwi_sample.tgz&#39; CIFAR = f&#39;{URL}cifar10.tgz&#39; COCO_SAMPLE = f&#39;{S3_COCO}coco_sample.tgz&#39; COCO_TINY = f&#39;{S3_COCO}coco_tiny.tgz&#39; HUMAN_NUMBERS = f&#39;{URL}human_numbers.tgz&#39; IMDB = f&#39;{S3_NLP}imdb.tgz&#39; IMDB_SAMPLE = f&#39;{URL}imdb_sample.tgz&#39; ML_SAMPLE = f&#39;{URL}movie_lens_sample.tgz&#39; ML_100k = &#39;https://files.grouplens.org/datasets/movielens/ml-100k.zip&#39; MNIST_SAMPLE = f&#39;{URL}mnist_sample.tgz&#39; MNIST_TINY = f&#39;{URL}mnist_tiny.tgz&#39; MNIST_VAR_SIZE_TINY = f&#39;{S3_IMAGE}mnist_var_size_tiny.tgz&#39; PLANET_SAMPLE = f&#39;{URL}planet_sample.tgz&#39; PLANET_TINY = f&#39;{URL}planet_tiny.tgz&#39; IMAGENETTE = f&#39;{S3_IMAGE}imagenette2.tgz&#39; IMAGENETTE_160 = f&#39;{S3_IMAGE}imagenette2-160.tgz&#39; IMAGENETTE_320 = f&#39;{S3_IMAGE}imagenette2-320.tgz&#39; IMAGEWOOF = f&#39;{S3_IMAGE}imagewoof2.tgz&#39; IMAGEWOOF_160 = f&#39;{S3_IMAGE}imagewoof2-160.tgz&#39; IMAGEWOOF_320 = f&#39;{S3_IMAGE}imagewoof2-320.tgz&#39; IMAGEWANG = f&#39;{S3_IMAGE}imagewang.tgz&#39; IMAGEWANG_160 = f&#39;{S3_IMAGE}imagewang-160.tgz&#39; IMAGEWANG_320 = f&#39;{S3_IMAGE}imagewang-320.tgz&#39; # kaggle competitions download dogs-vs-cats -p {DOGS.absolute()} DOGS = f&#39;{URL}dogscats.tgz&#39; # image classification datasets CALTECH_101 = f&#39;{S3_IMAGE}caltech_101.tgz&#39; CARS = f&#39;{S3_IMAGE}stanford-cars.tgz&#39; CIFAR_100 = f&#39;{S3_IMAGE}cifar100.tgz&#39; CUB_200_2011 = f&#39;{S3_IMAGE}CUB_200_2011.tgz&#39; FLOWERS = f&#39;{S3_IMAGE}oxford-102-flowers.tgz&#39; FOOD = f&#39;{S3_IMAGE}food-101.tgz&#39; MNIST = f&#39;{S3_IMAGE}mnist_png.tgz&#39; PETS = f&#39;{S3_IMAGE}oxford-iiit-pet.tgz&#39; # NLP datasets AG_NEWS = f&#39;{S3_NLP}ag_news_csv.tgz&#39; AMAZON_REVIEWS = f&#39;{S3_NLP}amazon_review_full_csv.tgz&#39; AMAZON_REVIEWS_POLARITY = f&#39;{S3_NLP}amazon_review_polarity_csv.tgz&#39; DBPEDIA = f&#39;{S3_NLP}dbpedia_csv.tgz&#39; MT_ENG_FRA = f&#39;{S3_NLP}giga-fren.tgz&#39; SOGOU_NEWS = f&#39;{S3_NLP}sogou_news_csv.tgz&#39; WIKITEXT = f&#39;{S3_NLP}wikitext-103.tgz&#39; WIKITEXT_TINY = f&#39;{S3_NLP}wikitext-2.tgz&#39; YAHOO_ANSWERS = f&#39;{S3_NLP}yahoo_answers_csv.tgz&#39; YELP_REVIEWS = f&#39;{S3_NLP}yelp_review_full_csv.tgz&#39; YELP_REVIEWS_POLARITY = f&#39;{S3_NLP}yelp_review_polarity_csv.tgz&#39; # Image localization datasets BIWI_HEAD_POSE = f&#34;{S3_IMAGELOC}biwi_head_pose.tgz&#34; CAMVID = f&#39;{S3_IMAGELOC}camvid.tgz&#39; CAMVID_TINY = f&#39;{URL}camvid_tiny.tgz&#39; LSUN_BEDROOMS = f&#39;{S3_IMAGE}bedroom.tgz&#39; PASCAL_2007 = f&#39;{S3_IMAGELOC}pascal_2007.tgz&#39; PASCAL_2012 = f&#39;{S3_IMAGELOC}pascal_2012.tgz&#39; # Audio classification datasets MACAQUES = f&#39;{GOOGLE}ml-animal-sounds-datasets/macaques.zip&#39; ZEBRA_FINCH = f&#39;{GOOGLE}ml-animal-sounds-datasets/zebra_finch.zip&#39; # Medical Imaging datasets #SKIN_LESION = f&#39;{S3_IMAGELOC}skin_lesion.tgz&#39; SIIM_SMALL = f&#39;{S3_IMAGELOC}siim_small.tgz&#39; TCGA_SMALL = f&#39;{S3_IMAGELOC}tcga_small.tgz&#39; #Pretrained models OPENAI_TRANSFORMER = f&#39;{S3_MODEL}transformer.tgz&#39; WT103_FWD = f&#39;{S3_MODEL}wt103-fwd.tgz&#39; WT103_BWD = f&#39;{S3_MODEL}wt103-bwd.tgz&#39; def path( url:str=&#39;.&#39;, # File to download c_key:str=&#39;archive&#39; # Key in `Config` where to save URL ) -&gt; Path: &#34;Local path where to download based on `c_key`&#34; fname = url.split(&#39;/&#39;)[-1] local_path = URLs.LOCAL_PATH/(&#39;models&#39; if c_key==&#39;model&#39; else &#39;data&#39;)/fname if local_path.exists(): return local_path return fastai_path(c_key)/fname File: ~/mambaforge/lib/python3.9/site-packages/fastai/data/external.py Type: type Subclasses: . path = untar_data(URLs.IMAGENETTE_160) . To access the image files, we can use get_image_files: . t = get_image_files(path) t[0] . Path(&#39;/root/.fastai/data/imagenette2-160/train/n02102040/n02102040_5405.JPEG&#39;) . Or we could do the same thing using just Python&#39;s standard library, with glob: . glob? . Type: module String form: &lt;module &#39;glob&#39; from &#39;/root/mambaforge/lib/python3.9/glob.py&#39;&gt; File: ~/mambaforge/lib/python3.9/glob.py Docstring: Filename globbing utility. . from glob import glob files = L(glob(f&#39;{path}/**/*.JPEG&#39;, recursive=True)).map(Path) files[0] . Path(&#39;/root/.fastai/data/imagenette2-160/train/n02102040/n02102040_5405.JPEG&#39;) . If you look at the source for get_image_files, you&#39;ll see it uses Python&#39;s os.walk; this is a faster and more flexible function than glob, so be sure to try it out. . get_image_files?? . Signature: get_image_files(path, recurse=True, folders=None) Source: def get_image_files(path, recurse=True, folders=None): &#34;Get image files in `path` recursively, only in `folders`, if specified.&#34; return get_files(path, extensions=image_extensions, recurse=recurse, folders=folders) File: ~/mambaforge/lib/python3.9/site-packages/fastai/data/transforms.py Type: function . get_files?? . Signature: get_files( path, extensions=None, recurse=True, folders=None, followlinks=True, ) Source: def get_files(path, extensions=None, recurse=True, folders=None, followlinks=True): &#34;Get all the files in `path` with optional `extensions`, optionally with `recurse`, only in `folders`, if specified.&#34; path = Path(path) folders=L(folders) extensions = setify(extensions) extensions = {e.lower() for e in extensions} if recurse: res = [] for i,(p,d,f) in enumerate(os.walk(path, followlinks=followlinks)): # returns (dirpath, dirnames, filenames) if len(folders) !=0 and i==0: d[:] = [o for o in d if o in folders] else: d[:] = [o for o in d if not o.startswith(&#39;.&#39;)] if len(folders) !=0 and i==0 and &#39;.&#39; not in folders: continue res += _get_files(p, f, extensions) else: f = [o.name for o in os.scandir(path) if o.is_file()] res = _get_files(path, f, extensions) return L(res) File: ~/mambaforge/lib/python3.9/site-packages/fastai/data/transforms.py Type: function . os.walk? . Signature: os.walk(top, topdown=True, onerror=None, followlinks=False) Docstring: Directory tree generator. For each directory in the directory tree rooted at top (including top itself, but excluding &#39;.&#39; and &#39;..&#39;), yields a 3-tuple dirpath, dirnames, filenames dirpath is a string, the path to the directory. dirnames is a list of the names of the subdirectories in dirpath (excluding &#39;.&#39; and &#39;..&#39;). filenames is a list of the names of the non-directory files in dirpath. Note that the names in the lists are just names, with no path components. To get a full path (which begins with top) to a file or directory in dirpath, do os.path.join(dirpath, name). If optional arg &#39;topdown&#39; is true or not specified, the triple for a directory is generated before the triples for any of its subdirectories (directories are generated top down). If topdown is false, the triple for a directory is generated after the triples for all of its subdirectories (directories are generated bottom up). When topdown is true, the caller can modify the dirnames list in-place (e.g., via del or slice assignment), and walk will only recurse into the subdirectories whose names remain in dirnames; this can be used to prune the search, or to impose a specific order of visiting. Modifying dirnames when topdown is false has no effect on the behavior of os.walk(), since the directories in dirnames have already been generated by the time dirnames itself is generated. No matter the value of topdown, the list of subdirectories is retrieved before the tuples for the directory and its subdirectories are generated. By default errors from the os.scandir() call are ignored. If optional arg &#39;onerror&#39; is specified, it should be a function; it will be called with one argument, an OSError instance. It can report the error to continue with the walk, or raise the exception to abort the walk. Note that the filename is available as the filename attribute of the exception object. By default, os.walk does not follow symbolic links to subdirectories on systems that support them. In order to get this functionality, set the optional argument &#39;followlinks&#39; to true. Caution: if you pass a relative pathname for top, don&#39;t change the current working directory between resumptions of walk. walk never changes the current directory, and assumes that the client doesn&#39;t either. Example: import os from os.path import join, getsize for root, dirs, files in os.walk(&#39;python/Lib/email&#39;): print(root, &#34;consumes&#34;, end=&#34;&#34;) print(sum(getsize(join(root, name)) for name in files), end=&#34;&#34;) print(&#34;bytes in&#34;, len(files), &#34;non-directory files&#34;) if &#39;CVS&#39; in dirs: dirs.remove(&#39;CVS&#39;) # don&#39;t visit CVS directories File: ~/mambaforge/lib/python3.9/os.py Type: function . L(os.walk(&#39;/&#39;)) . KeyboardInterrupt Traceback (most recent call last) Input In [6], in &lt;cell line: 1&gt;() -&gt; 1 L(os.walk(&#39;/&#39;)) File ~/mambaforge/lib/python3.9/site-packages/fastcore/foundation.py:97, in _L_Meta.__call__(cls, x, *args, **kwargs) 95 def __call__(cls, x=None, *args, **kwargs): 96 if not args and not kwargs and x is not None and isinstance(x,cls): return x &gt; 97 return super().__call__(x, *args, **kwargs) File ~/mambaforge/lib/python3.9/site-packages/fastcore/foundation.py:105, in L.__init__(self, items, use_list, match, *rest) 103 def __init__(self, items=None, *rest, use_list=False, match=None): 104 if (use_list is not None) or not is_array(items): --&gt; 105 items = listify(items, *rest, use_list=use_list, match=match) 106 super().__init__(items) File ~/mambaforge/lib/python3.9/site-packages/fastcore/basics.py:59, in listify(o, use_list, match, *rest) 57 elif isinstance(o, list): res = o 58 elif isinstance(o, str) or is_array(o): res = [o] &gt; 59 elif is_iter(o): res = list(o) 60 else: res = [o] 61 if match is not None: File ~/mambaforge/lib/python3.9/os.py:418, in _walk(top, topdown, onerror, followlinks) 413 # Issue #23605: os.path.islink() is used instead of caching 414 # entry.is_symlink() result during the loop on os.scandir() because 415 # the caller can replace the directory entry during the &#34;yield&#34; 416 # above. 417 if followlinks or not islink(new_path): --&gt; 418 yield from _walk(new_path, topdown, onerror, followlinks) 419 else: 420 # Recurse into sub-directories 421 for new_path in walk_dirs: File ~/mambaforge/lib/python3.9/os.py:418, in _walk(top, topdown, onerror, followlinks) 413 # Issue #23605: os.path.islink() is used instead of caching 414 # entry.is_symlink() result during the loop on os.scandir() because 415 # the caller can replace the directory entry during the &#34;yield&#34; 416 # above. 417 if followlinks or not islink(new_path): --&gt; 418 yield from _walk(new_path, topdown, onerror, followlinks) 419 else: 420 # Recurse into sub-directories 421 for new_path in walk_dirs: [... skipping similar frames: _walk at line 418 (9 times)] File ~/mambaforge/lib/python3.9/os.py:418, in _walk(top, topdown, onerror, followlinks) 413 # Issue #23605: os.path.islink() is used instead of caching 414 # entry.is_symlink() result during the loop on os.scandir() because 415 # the caller can replace the directory entry during the &#34;yield&#34; 416 # above. 417 if followlinks or not islink(new_path): --&gt; 418 yield from _walk(new_path, topdown, onerror, followlinks) 419 else: 420 # Recurse into sub-directories 421 for new_path in walk_dirs: File ~/mambaforge/lib/python3.9/os.py:367, in _walk(top, topdown, onerror, followlinks) 365 try: 366 try: --&gt; 367 entry = next(scandir_it) 368 except StopIteration: 369 break KeyboardInterrupt: . We can open an image with the Python Imaging Library&#39;s Image class: . Image.open? . Signature: Image.open(fp, mode=&#39;r&#39;, formats=None) Docstring: Opens and identifies the given image file. This is a lazy operation; this function identifies the file, but the file remains open and the actual image data is not read from the file until you try to process the data (or call the :py:meth:`~PIL.Image.Image.load` method). See :py:func:`~PIL.Image.new`. See :ref:`file-handling`. :param fp: A filename (string), pathlib.Path object or a file object. The file object must implement ``file.read``, ``file.seek``, and ``file.tell`` methods, and be opened in binary mode. :param mode: The mode. If given, this argument must be &#34;r&#34;. :param formats: A list or tuple of formats to attempt to load the file in. This can be used to restrict the set of formats checked. Pass ``None`` to try all supported formats. You can print the set of available formats by running ``python3 -m PIL`` or using the :py:func:`PIL.features.pilinfo` function. :returns: An :py:class:`~PIL.Image.Image` object. :exception FileNotFoundError: If the file cannot be found. :exception PIL.UnidentifiedImageError: If the image cannot be opened and identified. :exception ValueError: If the ``mode`` is not &#34;r&#34;, or if a ``StringIO`` instance is used for ``fp``. :exception TypeError: If ``formats`` is not ``None``, a list or a tuple. File: ~/mambaforge/lib/python3.9/site-packages/PIL/Image.py Type: function . im = Image.open(files[0]) im . im_t = tensor(im) im_t.shape . torch.Size([240, 160, 3]) . That&#39;s going to be the basis of our independent variable. For our dependent variable, we can use Path.parent from pathlib. First we&#39;ll need our vocab: . files[0] . Path(&#39;/root/.fastai/data/imagenette2-160/train/n02102040/n02102040_5405.JPEG&#39;) . files[0].parent . Path(&#39;/root/.fastai/data/imagenette2-160/train/n02102040&#39;) . files[0].parent.name . &#39;n02102040&#39; . files.map(lambda x: x.parent.name).unique() . (#10) [&#39;n02102040&#39;,&#39;n03445777&#39;,&#39;n03394916&#39;,&#39;n03425413&#39;,&#39;n03000684&#39;,&#39;n01440764&#39;,&#39;n03888257&#39;,&#39;n03417042&#39;,&#39;n02979186&#39;,&#39;n03028079&#39;] . files.map(lambda x: x.parent.name).unique().val2idx?? . Signature: val2idx(x) Source: def val2idx(x): &#34;Dict from value to index&#34; return {v:k for k,v in enumerate(x)} File: ~/mambaforge/lib/python3.9/site-packages/fastcore/basics.py Type: function . {v:i for i, v in enumerate(files.map(lambda x: x.parent.name).unique())} . {&#39;n02102040&#39;: 0, &#39;n03445777&#39;: 1, &#39;n03394916&#39;: 2, &#39;n03425413&#39;: 3, &#39;n03000684&#39;: 4, &#39;n01440764&#39;: 5, &#39;n03888257&#39;: 6, &#39;n03417042&#39;: 7, &#39;n02979186&#39;: 8, &#39;n03028079&#39;: 9} . lbls = files.map(Self.parent.name()).unique(); lbls . (#10) [&#39;n02102040&#39;,&#39;n03445777&#39;,&#39;n03394916&#39;,&#39;n03425413&#39;,&#39;n03000684&#39;,&#39;n01440764&#39;,&#39;n03888257&#39;,&#39;n03417042&#39;,&#39;n02979186&#39;,&#39;n03028079&#39;] . ...and the reverse mapping, thanks to L.val2idx: . v2i = lbls.val2idx(); v2i . {&#39;n02102040&#39;: 0, &#39;n03445777&#39;: 1, &#39;n03394916&#39;: 2, &#39;n03425413&#39;: 3, &#39;n03000684&#39;: 4, &#39;n01440764&#39;: 5, &#39;n03888257&#39;: 6, &#39;n03417042&#39;: 7, &#39;n02979186&#39;: 8, &#39;n03028079&#39;: 9} . That&#39;s all the pieces we need to put together our Dataset. . Dataset . A Dataset in PyTorch can be anything that supports indexing (__getitem__) and len: . class Dataset: def __init__(self, fns): self.fns=fns def __len__(self): return len(self.fns) def __getitem__(self, i): im = Image.open(self.fns[i]).resize((64,64)).convert(&#39;RGB&#39;) y = v2i[self.fns[i].parent.name] return tensor(im).float()/255, tensor(y) . dset = Dataset(get_image_files(path)[:3]) len(dset) . 3 . X, y = dset[0] X.shape, y . (torch.Size([64, 64, 3]), tensor(0)) . We need a list of training and validation filenames to pass to Dataset.__init__: . set(o.parent.parent.name for o in files) . {&#39;train&#39;, &#39;val&#39;} . train_filt = L(o.parent.parent.name==&#39;train&#39; for o in files) train,valid = files[train_filt],files[~train_filt] len(train),len(valid) . (9469, 3925) . Now we can try it out: . train_ds,valid_ds = Dataset(train),Dataset(valid) x,y = train_ds[0] x.shape,y . (torch.Size([64, 64, 3]), tensor(0)) . show_image(x, title=lbls[y]); . As you see, our dataset is returning the independent and dependent variables as a tuple, which is just what we need. We&#39;ll need to be able to collate these into a mini-batch. Generally this is done with torch.stack, which is what we&#39;ll use here: . tmp = [(o, i) for i, o in enumerate(&quot;A B C D E F&quot;.split())] a, b = zip(*tmp) a, b . ((&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;D&#39;, &#39;E&#39;, &#39;F&#39;), (0, 1, 2, 3, 4, 5)) . def collate(idxs, ds): xb,yb = zip(*[ds[i] for i in idxs]) return torch.stack(xb),torch.stack(yb) . class Dataset0: def __init__(self, l): self.l=l def __len__(self): return len(self.l) def __getitem__(self, i): return torch.tensor(ord(self.l[i][0])), torch.tensor(self.l[i][1]) collate([0,1,2], Dataset0([(o, i) for i, o in enumerate(&quot;a b c d e f&quot;.split())])) . (tensor([97, 98, 99]), tensor([0, 1, 2])) . Here&#39;s a mini-batch with two items, for testing our collate: . x,y = collate([1,2], train_ds) x.shape,y . (torch.Size([2, 64, 64, 3]), tensor([0, 0])) . Now that we have a dataset and a collation function, we&#39;re ready to create DataLoader. We&#39;ll add two more things here: an optional shuffle for the training set, and a ProcessPoolExecutor to do our preprocessing in parallel. A parallel data loader is very important, because opening and decoding a JPEG image is a slow process. One CPU core is not enough to decode images fast enough to keep a modern GPU busy. Here&#39;s our DataLoader class: . L.range(9).shuffle() . (#9) [3,6,7,4,8,2,5,0,1] . class DataLoader: def __init__(self, ds, bs=128, shuffle=False, n_workers=1): self.ds,self.bs,self.shuffle,self.n_workers = ds,bs,shuffle,n_workers def __len__(self): return (len(self.ds)-1)//self.bs+1 def __iter__(self): idxs = L.range(self.ds) if self.shuffle: idxs = idxs.shuffle() chunks = [idxs[n:n+self.bs] for n in range(0, len(self.ds), self.bs)] with ProcessPoolExecutor(self.n_workers) as ex: yield from ex.map(collate, chunks, ds=self.ds) . Let&#39;s try it out with our training and validation datasets: . defaults.cpus . 8 . n_workers = min(16, defaults.cpus) %time train_dl = DataLoader(train_ds, bs=128, shuffle=True, n_workers=n_workers) valid_dl = DataLoader(valid_ds, bs=256, shuffle=False, n_workers=n_workers) xb,yb = first(train_dl) xb.shape,yb.shape,len(train_dl) . CPU times: user 7 µs, sys: 4 µs, total: 11 µs Wall time: 15.7 µs . (torch.Size([128, 64, 64, 3]), torch.Size([128]), 74) . This data loader is not much slower than PyTorch&#39;s, but it&#39;s far simpler. So if you&#39;re debugging a complex data loading process, don&#39;t be afraid to try doing things manually to help you see exactly what&#39;s going on. . For normalization, we&#39;ll need image statistics. Generally it&#39;s fine to calculate these on a single training mini-batch, since precision isn&#39;t needed here: . stats = [xb.mean((0,1,2)),xb.std((0,1,2))] stats . [tensor([0.4595, 0.4560, 0.4263]), tensor([0.2685, 0.2648, 0.2886])] . Our Normalize class just needs to store these stats and apply them (to see why the to_device is needed, try commenting it out, and see what happens later in this notebook): . class Normalize: def __init__(self, stats): self.stats=stats def __call__(self, x): if x.device != self.stats[0].device: # self.stats = to_device(self.stats, x.device) print(x.device, self.stats[0].device) return (x-self.stats[0])/self.stats[1] . We always like to test everything we build in a notebook, as soon as we build it: . torch.arange(2*3*4*5).reshape(2,3,4,5).permute((0,3,1,2)).shape # swap axises . torch.Size([2, 5, 3, 4]) . x.shape, x.permute(0,3,1,2).shape # -&gt; (N, C, H, W) . (torch.Size([2, 64, 64, 3]), torch.Size([2, 3, 64, 64])) . norm = Normalize(stats) def tfm_x(x): return norm(x).permute((0,3,1,2)) . t = tfm_x(x) t.mean((0,2,3)),t.std((0,2,3)) . (tensor([-0.1056, -0.1291, 0.1143]), tensor([0.8184, 0.8226, 0.7869])) . Here tfm_x isn&#39;t just applying Normalize, but is also permuting the axis order from NHWC to NCHW (see &lt;&gt; if you need a reminder of what these acronyms refer to).&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; PIL uses HWC axis order, which we can&#39;t use with PyTorch, hence the need for this permute. . That&#39;s all we need for the data for our model. So now we need the model itself! . Module and Parameter . To create a model, we&#39;ll need Module. To create Module, we&#39;ll need Parameter, so let&#39;s start there. Recall that in &lt;&gt; we said that the Parameter class &quot;doesn&#39;t actually add any functionality (other than automatically calling requires_grad_ for us). It&#39;s only used as a &quot;marker&quot; to show what to include in parameters.&quot; Here&#39;s a definition which does exactly that:&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; class Parameter(Tensor): def __new__(self, x): return Tensor._make_subclass(Parameter, x, True) def __init__(self, *args, **kwargs): self.requires_grad_() . The implementation here is a bit awkward: we have to define the special __new__ Python method and use the internal PyTorch method _make_subclass because, as at the time of writing, PyTorch doesn&#39;t otherwise work correctly with this kind of subclassing or provide an officially supported API to do this. This may have been fixed by the time you read this, so look on the book&#39;s website to see if there are updated details. . Our Parameter now behaves just like a tensor, as we wanted: . Parameter(tensor(3.)) . tensor(3., requires_grad=True) . Now that we have this, we can define Module: . class Module: def __init__(self): self.hook,self.params,self.children,self._training = None,[],[],False def register_parameters(self, *ps): self.params += ps def register_modules (self, *ms): self.children += ms @property def training(self): return self._training @training.setter def training(self,v): self._training = v for m in self.children: m.training=v def parameters(self): return self.params + sum([m.parameters() for m in self.children], []) def __setattr__(self,k,v): super().__setattr__(k,v) if isinstance(v,Parameter): self.register_parameters(v) if isinstance(v,Module): self.register_modules(v) def __call__(self, *args, **kwargs): res = self.forward(*args, **kwargs) if self.hook is not None: self.hook(res, args) return res def cuda(self): for p in self.parameters(): p.data = p.data.cuda() . The key functionality is in the definition of parameters: . self.params + sum([m.parameters() for m in self.children], []) . This means that we can ask any Module for its parameters, and it will return them, including all its child modules (recursively). But how does it know what its parameters are? It&#39;s thanks to implementing Python&#39;s special __setattr__ method, which is called for us any time Python sets an attribute on a class. Our implementation includes this line: . if isinstance(v,Parameter): self.register_parameters(v) . As you see, this is where we use our new Parameter class as a &quot;marker&quot;—anything of this class is added to our params. . Python&#39;s __call__ allows us to define what happens when our object is treated as a function; we just call forward (which doesn&#39;t exist here, so it&#39;ll need to be added by subclasses). Before we do, we&#39;ll call a hook, if it&#39;s defined. Now you can see that PyTorch hooks aren&#39;t doing anything fancy at all—they&#39;re just calling any hooks that have been registered. . Other than these pieces of functionality, our Module also provides cuda and training attributes, which we&#39;ll use shortly. . Now we can create our first Module, which is ConvLayer: . class ConvLayer(Module): def __init__(self, ni, nf, stride=1, bias=True, act=True): super().__init__() self.w = Parameter(torch.zeros(nf,ni,3,3)) self.b = Parameter(torch.zeros(nf)) if bias else None self.act,self.stride = act,stride init = nn.init.kaiming_normal_ if act else nn.init.xavier_normal_ init(self.w) def forward(self, x): x = F.conv2d(x, self.w, self.b, stride=self.stride, padding=1) if self.act: x = F.relu(x) return x . We&#39;re not implementing F.conv2d from scratch, since you should have already done that (using unfold) in the questionnaire in &lt;&gt;. Instead, we&#39;re just creating a small class that wraps it up along with bias and weight initialization. Let&#39;s check that it works correctly with Module.parameters:&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; l = ConvLayer(3, 4) l.parameters()[0].shape, l.parameters()[1].shape . (torch.Size([4, 3, 3, 3]), torch.Size([4])) . And that we can call it (which will result in forward being called): . xbt = tfm_x(xb) xb.shape, xbt.shape . (torch.Size([128, 64, 64, 3]), torch.Size([128, 3, 64, 64])) . r = l(xbt) r.shape . torch.Size([128, 4, 64, 64]) . In the same way, we can implement Linear: . class Linear(Module): def __init__(self, ni, nf): super().__init__() self.w = Parameter(torch.zeros(nf,ni)) self.b = Parameter(torch.zeros(nf)) nn.init.xavier_normal_(self.w) def forward(self, x): return x@self.w.t() + self.b . and test if it works: . l = Linear(4,2) r = l(torch.ones(3,4)) r.shape . torch.Size([3, 2]) . Let&#39;s also create a testing module to check that if we include multiple parameters as attributes, they are all correctly registered: . class T(Module): def __init__(self): super().__init__() self.c,self.l = ConvLayer(3,4),Linear(4,2) . Since we have a conv layer and a linear layer, each of which has weights and biases, we&#39;d expect four parameters in total: . t = T() . t.children, len(t.children[0].parameters()), len(t.children[1].parameters()) . ([&lt;__main__.ConvLayer at 0x7f9518d16520&gt;, &lt;__main__.Linear at 0x7f95192ae3a0&gt;], 2, 2) . len(t.parameters()) . 4 . We should also find that calling cuda on this class puts all these parameters on the GPU: . t.l.w.device . device(type=&#39;cpu&#39;) . t.cuda() t.l.w.device . device(type=&#39;cuda&#39;, index=0) . We can now use those pieces to create a CNN. . Simple CNN . As we&#39;ve seen, a Sequential class makes many architectures easier to implement, so let&#39;s make one: . class Sequential(Module): def __init__(self, *layers): super().__init__() self.layers = layers self.register_modules(*layers) def forward(self, x): for l in self.layers: x = l(x) return x . The forward method here just calls each layer in turn. Note that we have to use the register_modules method we defined in Module, since otherwise the contents of layers won&#39;t appear in parameters. . . Important: All The Code is Here: Remember that we&#8217;re not using any PyTorch functionality for modules here; we&#8217;re defining everything ourselves. So if you&#8217;re not sure what register_modules does, or why it&#8217;s needed, have another look at our code for Module to see what we wrote! . We can create a simplified AdaptivePool that only handles pooling to a 1×1 output, and flattens it as well, by just using mean: . class AdaptivePool(Module): def forward(self, x): return x.mean((2,3)) . That&#39;s enough for us to create a CNN! . def simple_cnn(): return Sequential( ConvLayer(3 ,16 ,stride=2), #32 ConvLayer(16,32 ,stride=2), #16 ConvLayer(32,64 ,stride=2), # 8 ConvLayer(64,128,stride=2), # 4 AdaptivePool(), Linear(128, 10) ) . Let&#39;s see if our parameters are all being registered correctly: . m = simple_cnn() len(m.parameters()) . 10 . [len(o.parameters()) for o in m.children] . [2, 2, 2, 2, 0, 2] . sum([len(o.parameters()) for o in m.children]) . 10 . Now we can try adding a hook. Note that we&#39;ve only left room for one hook in Module; you could make it a list, or use something like Pipeline to run a few as a single function: . def print_stats(outp, inp): print (outp.mean().item(),outp.std().item()) for i in range(4): m.layers[i].hook = print_stats r = m(xbt) r.shape . 0.5291942358016968 0.8697973489761353 0.4359581172466278 0.825819730758667 0.4345751404762268 0.7494376301765442 0.46103182435035706 0.7244757413864136 . torch.Size([128, 10]) . m.layers . (&lt;__main__.ConvLayer at 0x7fec32275220&gt;, &lt;__main__.ConvLayer at 0x7fec32275460&gt;, &lt;__main__.ConvLayer at 0x7fec32275d00&gt;, &lt;__main__.ConvLayer at 0x7fec32275e20&gt;, &lt;__main__.AdaptivePool at 0x7fec32268d30&gt;, &lt;__main__.Linear at 0x7fec322754f0&gt;) . m.children . [&lt;__main__.ConvLayer at 0x7fec32275220&gt;, &lt;__main__.ConvLayer at 0x7fec32275460&gt;, &lt;__main__.ConvLayer at 0x7fec32275d00&gt;, &lt;__main__.ConvLayer at 0x7fec32275e20&gt;, &lt;__main__.AdaptivePool at 0x7fec32268d30&gt;, &lt;__main__.Linear at 0x7fec322754f0&gt;] . m . &lt;__main__.Sequential at 0x7fec32275af0&gt; . def print_stats(outp, inp): print (outp.mean().item(),outp.std().item()) for o in m.layers: o.hook = print_stats r = m(xbt) r.shape . 0.5291942358016968 0.8697973489761353 0.4359581172466278 0.825819730758667 0.4345751404762268 0.7494376301765442 0.46103182435035706 0.7244757413864136 0.46103185415267944 0.5164786577224731 0.4029931128025055 0.9249605536460876 . torch.Size([128, 10]) . def print_stats(outp, inp): if self.act: print(act) for o in m.layers: o.hook = print_stats r = m(xbt) r.shape . We have data and model. Now we need a loss function. . Loss . We&#39;ve already seen how to define &quot;negative log likelihood&quot;: . def nll(input, target): return -input[range(target.shape[0]), target].mean() . Well actually, there&#39;s no log here, since we&#39;re using the same definition as PyTorch. That means we need to put the log together with softmax: . batch_size = 5 n_classes = 3 input = torch.randn(batch_size, n_classes).relu() input . tensor([[0.0000, 0.0000, 0.0000], [0.8550, 0.0000, 0.5435], [0.0000, 2.5415, 0.0000], [0.0000, 1.0593, 0.0000], [0.2269, 0.0000, 0.0000]]) . target = torch.randint(0, n_classes, (batch_size,)) target . tensor([1, 0, 0, 2, 2]) . -input[range(batch_size), target] . tensor([-0.0000, -0.8550, -0.0000, -0.0000, -0.0000]) . -input[range(batch_size), target].mean() . tensor(-0.1710) . nll(input, target) . tensor(-0.1710) . Well actually, there&#39;s no log here, since we&#39;re using the same definition as PyTorch. That means we need to put the log together with softmax: . def log_softmax(x): return (x.exp()/(x.exp().sum(-1,keepdim=True))).log() sm = log_softmax(r); sm[0][0] . tensor(-1.8825, grad_fn=&lt;AliasBackward0&gt;) . Combining these gives us our cross-entropy loss: . loss = nll(sm, yb) loss . tensor(2.4390, grad_fn=&lt;AliasBackward0&gt;) . Note that the formula: . $$ log left ( frac{a}{b} right ) = log(a) - log(b)$$ . gives a simplification when we compute the log softmax, which was previously defined as (x.exp()/(x.exp().sum(-1))).log(): . def log_softmax(x): return x - x.exp().sum(-1,keepdim=True).log() sm = log_softmax(r); sm[0][0] . tensor(-0.0448, grad_fn=&lt;AliasBackward0&gt;) . Then, there is a more stable way to compute the log of the sum of exponentials, called the LogSumExp trick. The idea is to use the following formula: . $$ log left ( sum_{j=1}^{n} e^{x_{j}} right ) = log left ( e^{a} sum_{j=1}^{n} e^{x_{j}-a} right ) = a + log left ( sum_{j=1}^{n} e^{x_{j}-a} right )$$ . where $a$ is the maximum of $x_{j}$. . Here&#39;s the same thing in code: . x = torch.rand(5) a = x.max() x.exp().sum().log() == a + (x-a).exp().sum().log() . tensor(True) . We&#39;ll put that into a function: . def logsumexp(x): m = x.max(-1)[0] return m + (x-m[:,None]).exp().sum(-1).log() logsumexp(r)[0] . tensor(0.9535, grad_fn=&lt;AliasBackward0&gt;) . so we can use it for our log_softmax function: . def log_softmax(x): return x - x.logsumexp(-1,keepdim=True) . Which gives the same result as before: . sm = log_softmax(r); sm[0][0] . tensor(-0.0448, grad_fn=&lt;AliasBackward0&gt;) . We can use these to create cross_entropy: . def cross_entropy(preds, yb): return nll(log_softmax(preds), yb).mean() . Let&#39;s now combine all those pieces together to create a Learner. . Learner . We have data, a model, and a loss function; we only need one more thing before we can fit a model, and that&#39;s an optimizer! Here&#39;s SGD: . class SGD: def __init__(self, params, lr, wd=0.): store_attr() def step(self): for p in self.params: p.data -= (p.grad.data + p.data*self.wd) * self.lr p.grad.data.zero_() . As we&#39;ve seen in this book, life is easier with a Learner. The Learner class needs to know our training and validation sets, which means we need DataLoaders to store them. We don&#39;t need any other functionality, just a place to store them and access them: . class DataLoaders: def __init__(self, *dls): self.train,self.valid = dls dls = DataLoaders(train_dl,valid_dl) . Now we&#39;re ready to create our Learner class: . store_attr?? . Signature: store_attr( names=None, self=None, but=&#39;&#39;, cast=False, store_args=None, **attrs, ) Source: def store_attr(names=None, self=None, but=&#39;&#39;, cast=False, store_args=None, **attrs): &#34;Store params named in comma-separated `names` from calling context into attrs in `self`&#34; fr = sys._getframe(1) args = argnames(fr, True) if self: args = (&#39;self&#39;, *args) else: self = fr.f_locals[args[0]] if store_args is None: store_args = not hasattr(self,&#39;__slots__&#39;) if store_args and not hasattr(self, &#39;__stored_args__&#39;): self.__stored_args__ = {} anno = annotations(self) if cast else {} if names and isinstance(names,str): names = re.split(&#39;, *&#39;, names) ns = names if names is not None else getattr(self, &#39;__slots__&#39;, args[1:]) added = {n:fr.f_locals[n] for n in ns} attrs = {**attrs, **added} if isinstance(but,str): but = re.split(&#39;, *&#39;, but) attrs = {k:v for k,v in attrs.items() if k not in but} return _store_attr(self, anno, **attrs) File: ~/mambaforge/lib/python3.9/site-packages/fastcore/basics.py Type: function . self? . Object `self` not found. . class Learner: def __init__(self, model, dls, loss_func, lr, cbs, opt_func=SGD): store_attr() for cb in cbs: cb.learner = self def one_batch(self): self(&#39;before_batch&#39;) xb,yb = self.batch self.preds = self.model(xb) self.loss = self.loss_func(self.preds, yb) if self.model.training: self.loss.backward() self.opt.step() self(&#39;after_batch&#39;) def one_epoch(self, train): self.model.training = train self(&#39;before_epoch&#39;) dl = self.dls.train if train else self.dls.valid #def progress_bar(dl, leave): return dl for self.num,self.batch in enumerate(progress_bar(dl, leave=False)): self.one_batch() self(&#39;after_epoch&#39;) def fit(self, n_epochs): self(&#39;before_fit&#39;) self.opt = self.opt_func(self.model.parameters(), self.lr) self.n_epochs = n_epochs try: for self.epoch in range(n_epochs): self.one_epoch(True) self.one_epoch(False) except CancelFitException: pass self(&#39;after_fit&#39;) def __call__(self,name): for cb in self.cbs: getattr(cb,name,noop)() . This is the largest class we&#39;ve created in the book, but each method is quite small, so by looking at each in turn you should be able to follow what&#39;s going on. . The main method we&#39;ll be calling is fit. This loops with: . for self.epoch in range(n_epochs) . and at each epoch calls self.one_epoch for each of train=True and then train=False. Then self.one_epoch calls self.one_batch for each batch in dls.train or dls.valid, as appropriate (after wrapping the DataLoader in fastprogress.progress_bar. Finally, self.one_batch follows the usual set of steps to fit one mini-batch that we&#39;ve seen throughout this book. . Before and after each step, Learner calls self, which calls __call__ (which is standard Python functionality). __call__ uses getattr(cb,name) on each callback in self.cbs, which is a Python built-in function that returns the attribute (a method, in this case) with the requested name. So, for instance, self(&#39;before_fit&#39;) will call cb.before_fit() for each callback where that method is defined. . As you can see, Learner is really just using our standard training loop, except that it&#39;s also calling callbacks at appropriate times. So let&#39;s define some callbacks! . Callbacks . In Learner.__init__ we have: . for cb in cbs: cb.learner = self . In other words, every callback knows what learner it is used in. This is critical, since otherwise a callback can&#39;t get information from the learner, or change things in the learner. Because getting information from the learner is so common, we make that easier by defining Callback as a subclass of GetAttr, with a default attribute of learner: . class Callback(GetAttr): _default=&#39;learner&#39; . GetAttr is a fastai class that implements Python&#39;s standard __getattr__ and __dir__ methods for you, such that any time you try to access an attribute that doesn&#39;t exist, it passes the request along to whatever you have defined as _default. . For instance, we want to move all model parameters to the GPU automatically at the start of fit. We could do this by defining before_fit as self.learner.model.cuda(); however, because learner is the default attribute, and we have SetupLearnerCB inherit from Callback (which inherits from GetAttr), we can remove the .learner and just call self.model.cuda(): . class SetupLearnerCB(Callback): def before_batch(self): xb,yb = to_device(self.batch) self.learner.batch = tfm_x(xb),yb def before_fit(self): self.model.cuda() . In SetupLearnerCB we also move each mini-batch to the GPU, by calling to_device(self.batch) (we could also have used the longer to_device(self.learner.batch). Note however that in the line self.learner.batch = tfm_x(xb),yb we can&#39;t remove .learner, because here we&#39;re setting the attribute, not getting it. . Before we try our Learner out, let&#39;s create a callback to track and print progress. Otherwise we won&#39;t really know if it&#39;s working properly: . class TrackResults(Callback): def before_epoch(self): self.accs,self.losses,self.ns = [],[],[] def after_epoch(self): n = sum(self.ns) print(self.epoch, self.model.training, sum(self.losses).item()/n, sum(self.accs).item()/n) def after_batch(self): xb,yb = self.batch acc = (self.preds.argmax(dim=1)==yb).float().sum() self.accs.append(acc) n = len(xb) self.losses.append(self.loss*n) self.ns.append(n) . Now we&#39;re ready to use our Learner for the first time! . cbs = [SetupLearnerCB(),TrackResults()] learn = Learner(simple_cnn(), dls, cross_entropy, lr=0.1, cbs=cbs) learn.fit(1) . . 0.00% [0/74 00:00&lt;00:00] KeyboardInterrupt Traceback (most recent call last) Input In [50], in DataLoader.__iter__(self) 11 with ProcessPoolExecutor(self.n_workers) as ex: &gt; 12 yield from ex.map(collate, chunks, ds=self.ds) File ~/mambaforge/lib/python3.9/concurrent/futures/process.py:562, in _chain_from_iterable_of_lists(iterable) 557 &#34;&#34;&#34; 558 Specialized implementation of itertools.chain.from_iterable. 559 Each item in *iterable* should be a list. This function is 560 careful not to keep references to yielded objects. 561 &#34;&#34;&#34; --&gt; 562 for element in iterable: 563 element.reverse() File ~/mambaforge/lib/python3.9/concurrent/futures/_base.py:609, in Executor.map.&lt;locals&gt;.result_iterator() 608 if timeout is None: --&gt; 609 yield fs.pop().result() 610 else: File ~/mambaforge/lib/python3.9/concurrent/futures/_base.py:441, in Future.result(self, timeout) 439 return self.__get_result() --&gt; 441 self._condition.wait(timeout) 443 if self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]: File ~/mambaforge/lib/python3.9/threading.py:312, in Condition.wait(self, timeout) 311 if timeout is None: --&gt; 312 waiter.acquire() 313 gotit = True KeyboardInterrupt: During handling of the above exception, another exception occurred: KeyboardInterrupt Traceback (most recent call last) Input In [103], in &lt;cell line: 3&gt;() 1 cbs = [SetupLearnerCB(),TrackResults()] 2 learn = Learner(simple_cnn(), dls, cross_entropy, lr=0.1, cbs=cbs) -&gt; 3 learn.fit(1) Input In [99], in Learner.fit(self, n_epochs) 29 try: 30 for self.epoch in range(n_epochs): &gt; 31 self.one_epoch(True) 32 self.one_epoch(False) 33 except CancelFitException: pass Input In [99], in Learner.one_epoch(self, train) 19 dl = self.dls.train if train else self.dls.valid 20 #def progress_bar(dl, leave): return dl &gt; 21 for self.num,self.batch in enumerate(progress_bar(dl, leave=False)): 22 self.one_batch() 23 self(&#39;after_epoch&#39;) File ~/mambaforge/lib/python3.9/site-packages/fastprogress/fastprogress.py:41, in ProgressBar.__iter__(self) 39 if self.total != 0: self.update(0) 40 try: &gt; 41 for i,o in enumerate(self.gen): 42 if i &gt;= self.total: break 43 yield o Input In [50], in DataLoader.__iter__(self) 10 chunks = [idxs[n:n+self.bs] for n in range(0, len(self.ds), self.bs)] 11 with ProcessPoolExecutor(self.n_workers) as ex: &gt; 12 yield from ex.map(collate, chunks, ds=self.ds) File ~/mambaforge/lib/python3.9/concurrent/futures/_base.py:637, in Executor.__exit__(self, exc_type, exc_val, exc_tb) 636 def __exit__(self, exc_type, exc_val, exc_tb): --&gt; 637 self.shutdown(wait=True) 638 return False File ~/mambaforge/lib/python3.9/concurrent/futures/process.py:767, in ProcessPoolExecutor.shutdown(self, wait, cancel_futures) 764 self._executor_manager_thread_wakeup.wakeup() 766 if self._executor_manager_thread is not None and wait: --&gt; 767 self._executor_manager_thread.join() 768 # To reduce the risk of opening too many files, remove references to 769 # objects that use file descriptors. 770 self._executor_manager_thread = None File ~/mambaforge/lib/python3.9/threading.py:1060, in Thread.join(self, timeout) 1057 raise RuntimeError(&#34;cannot join current thread&#34;) 1059 if timeout is None: -&gt; 1060 self._wait_for_tstate_lock() 1061 else: 1062 # the behavior of a negative timeout isn&#39;t documented, but 1063 # historically .join(timeout=x) for x&lt;0 has acted as if timeout=0 1064 self._wait_for_tstate_lock(timeout=max(timeout, 0)) File ~/mambaforge/lib/python3.9/threading.py:1080, in Thread._wait_for_tstate_lock(self, block, timeout) 1077 return 1079 try: -&gt; 1080 if lock.acquire(block, timeout): 1081 lock.release() 1082 self._stop() KeyboardInterrupt: . It&#39;s quite amazing to realize that we can implement all the key ideas from fastai&#39;s Learner in so little code! Let&#39;s now add some learning rate scheduling. . Scheduling the Learning Rate . If we&#39;re going to get good results, we&#39;ll want an LR finder and 1cycle training. These are both annealing callbacks—that is, they are gradually changing hyperparameters as we train. Here&#39;s LRFinder: . class LRFinder(Callback): def before_fit(self): self.losses,self.lrs = [],[] self.learner.lr = 1e-6 def before_batch(self): if not self.model.training: return self.opt.lr *= 1.2 def after_batch(self): if not self.model.training: return if self.opt.lr&gt;10 or torch.isnan(self.loss): raise CancelFitException self.losses.append(self.loss.item()) self.lrs.append(self.opt.lr) . This shows how we&#39;re using CancelFitException, which is itself an empty class, only used to signify the type of exception. You can see in Learner that this exception is caught. (You should add and test CancelBatchException, CancelEpochException, etc. yourself.) Let&#39;s try it out, by adding it to our list of callbacks: . lrfind = LRFinder() learn = Learner(simple_cnn(), dls, cross_entropy, lr=0.1, cbs=cbs+[lrfind]) learn.fit(2) . . 0.00% [0/74 00:00&lt;00:00] KeyboardInterrupt Traceback (most recent call last) Input In [50], in DataLoader.__iter__(self) 11 with ProcessPoolExecutor(self.n_workers) as ex: &gt; 12 yield from ex.map(collate, chunks, ds=self.ds) File ~/mambaforge/lib/python3.9/concurrent/futures/process.py:562, in _chain_from_iterable_of_lists(iterable) 557 &#34;&#34;&#34; 558 Specialized implementation of itertools.chain.from_iterable. 559 Each item in *iterable* should be a list. This function is 560 careful not to keep references to yielded objects. 561 &#34;&#34;&#34; --&gt; 562 for element in iterable: 563 element.reverse() File ~/mambaforge/lib/python3.9/concurrent/futures/_base.py:609, in Executor.map.&lt;locals&gt;.result_iterator() 608 if timeout is None: --&gt; 609 yield fs.pop().result() 610 else: File ~/mambaforge/lib/python3.9/concurrent/futures/_base.py:441, in Future.result(self, timeout) 439 return self.__get_result() --&gt; 441 self._condition.wait(timeout) 443 if self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]: File ~/mambaforge/lib/python3.9/threading.py:312, in Condition.wait(self, timeout) 311 if timeout is None: --&gt; 312 waiter.acquire() 313 gotit = True KeyboardInterrupt: During handling of the above exception, another exception occurred: KeyboardInterrupt Traceback (most recent call last) Input In [105], in &lt;cell line: 3&gt;() 1 lrfind = LRFinder() 2 learn = Learner(simple_cnn(), dls, cross_entropy, lr=0.1, cbs=cbs+[lrfind]) -&gt; 3 learn.fit(2) Input In [99], in Learner.fit(self, n_epochs) 29 try: 30 for self.epoch in range(n_epochs): &gt; 31 self.one_epoch(True) 32 self.one_epoch(False) 33 except CancelFitException: pass Input In [99], in Learner.one_epoch(self, train) 19 dl = self.dls.train if train else self.dls.valid 20 #def progress_bar(dl, leave): return dl &gt; 21 for self.num,self.batch in enumerate(progress_bar(dl, leave=False)): 22 self.one_batch() 23 self(&#39;after_epoch&#39;) File ~/mambaforge/lib/python3.9/site-packages/fastprogress/fastprogress.py:41, in ProgressBar.__iter__(self) 39 if self.total != 0: self.update(0) 40 try: &gt; 41 for i,o in enumerate(self.gen): 42 if i &gt;= self.total: break 43 yield o Input In [50], in DataLoader.__iter__(self) 10 chunks = [idxs[n:n+self.bs] for n in range(0, len(self.ds), self.bs)] 11 with ProcessPoolExecutor(self.n_workers) as ex: &gt; 12 yield from ex.map(collate, chunks, ds=self.ds) File ~/mambaforge/lib/python3.9/concurrent/futures/_base.py:637, in Executor.__exit__(self, exc_type, exc_val, exc_tb) 636 def __exit__(self, exc_type, exc_val, exc_tb): --&gt; 637 self.shutdown(wait=True) 638 return False File ~/mambaforge/lib/python3.9/concurrent/futures/process.py:767, in ProcessPoolExecutor.shutdown(self, wait, cancel_futures) 764 self._executor_manager_thread_wakeup.wakeup() 766 if self._executor_manager_thread is not None and wait: --&gt; 767 self._executor_manager_thread.join() 768 # To reduce the risk of opening too many files, remove references to 769 # objects that use file descriptors. 770 self._executor_manager_thread = None File ~/mambaforge/lib/python3.9/threading.py:1060, in Thread.join(self, timeout) 1057 raise RuntimeError(&#34;cannot join current thread&#34;) 1059 if timeout is None: -&gt; 1060 self._wait_for_tstate_lock() 1061 else: 1062 # the behavior of a negative timeout isn&#39;t documented, but 1063 # historically .join(timeout=x) for x&lt;0 has acted as if timeout=0 1064 self._wait_for_tstate_lock(timeout=max(timeout, 0)) File ~/mambaforge/lib/python3.9/threading.py:1080, in Thread._wait_for_tstate_lock(self, block, timeout) 1077 return 1079 try: -&gt; 1080 if lock.acquire(block, timeout): 1081 lock.release() 1082 self._stop() KeyboardInterrupt: . And take a look at the results: . plt.plot(lrfind.lrs[:-2],lrfind.losses[:-2]) plt.xscale(&#39;log&#39;) . Now we can define our OneCycle training callback: . class OneCycle(Callback): def __init__(self, base_lr): self.base_lr = base_lr def before_fit(self): self.lrs = [] def before_batch(self): if not self.model.training: return n = len(self.dls.train) bn = self.epoch*n + self.num mn = self.n_epochs*n pct = bn/mn pct_start,div_start = 0.25,10 if pct&lt;pct_start: pct /= pct_start lr = (1-pct)*self.base_lr/div_start + pct*self.base_lr else: pct = (pct-pct_start)/(1-pct_start) lr = (1-pct)*self.base_lr self.opt.lr = lr self.lrs.append(lr) . We&#39;ll try an LR of 0.1: . onecyc = OneCycle(0.1) learn = Learner(simple_cnn(), dls, cross_entropy, lr=0.1, cbs=cbs+[onecyc]) . Let&#39;s fit for a while and see how it looks (we won&#39;t show all the output in the book—try it in the notebook to see the results): . learn.fit(8) . Finally, we&#39;ll check that the learning rate followed the schedule we defined (as you see, we&#39;re not using cosine annealing here): . plt.plot(onecyc.lrs); . Conclusion . We have explored how the key concepts of the fastai library are implemented by re-implementing them in this chapter. Since it&#39;s mostly full of code, you should definitely try to experiment with it by looking at the corresponding notebook on the book&#39;s website. Now that you know how it&#39;s built, as a next step be sure to check out the intermediate and advanced tutorials in the fastai documentation to learn how to customize every bit of the library. . Questionnaire . . Tip: Experiments: For the questions here that ask you to explain what some function or class is, you should also complete your own code experiments. . What is glob? | How do you open an image with the Python imaging library? | What does L.map do? | What does Self do? | What is L.val2idx? | What methods do you need to implement to create your own Dataset? | Why do we call convert when we open an image from Imagenette? | What does ~ do? How is it useful for splitting training and validation sets? | Does ~ work with the L or Tensor classes? What about NumPy arrays, Python lists, or pandas DataFrames? | What is ProcessPoolExecutor? | How does L.range(self.ds) work? | What is __iter__? | What is first? | What is permute? Why is it needed? | What is a recursive function? How does it help us define the parameters method? | Write a recursive function that returns the first 20 items of the Fibonacci sequence. | What is super? | Why do subclasses of Module need to override forward instead of defining __call__? | In ConvLayer, why does init depend on act? | Why does Sequential need to call register_modules? | Write a hook that prints the shape of every layer&#39;s activations. | What is &quot;LogSumExp&quot;? | Why is log_softmax useful? | What is GetAttr? How is it helpful for callbacks? | Reimplement one of the callbacks in this chapter without inheriting from Callback or GetAttr. | What does Learner.__call__ do? | What is getattr? (Note the case difference to GetAttr!) | Why is there a try block in fit? | Why do we check for model.training in one_batch? | What is store_attr? | What is the purpose of TrackResults.before_epoch? | What does model.cuda do? How does it work? | Why do we need to check model.training in LRFinder and OneCycle? | Use cosine annealing in OneCycle. | Further Research . Write resnet18 from scratch (refer to &lt;&gt; as needed), and train it with the Learner in this chapter.&lt;/li&gt; Implement a batchnorm layer from scratch and use it in your resnet18. | Write a Mixup callback for use in this chapter. | Add momentum to SGD. | Pick a few features that you&#39;re interested in from fastai (or any other library) and implement them in this chapter. | Pick a research paper that&#39;s not yet implemented in fastai or PyTorch and implement it in this chapter. Port it over to fastai. | Submit a pull request to fastai, or create your own extension module and release it. | Hint: you may find it helpful to use nbdev to create and deploy your package. | . | &lt;/ol&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; | .",
            "url": "https://doyu.github.io/blog/2022/09/01/fastbook19_implmenet_learner.html",
            "relUrl": "/2022/09/01/fastbook19_implmenet_learner.html",
            "date": " • Sep 1, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Deep Learning from scratch with pytorch (1/N)",
            "content": "from fastai.vision.all import * . path = untar_data(URLs.MNIST_SAMPLE) Path.BASE_PATH = path path.ls() . (#3) [Path(&#39;train&#39;),Path(&#39;labels.csv&#39;),Path(&#39;valid&#39;)] . images = (path/&#39;train&#39;/&#39;3&#39;).ls() images . (#6131) [Path(&#39;train/3/55705.png&#39;),Path(&#39;train/3/32379.png&#39;),Path(&#39;train/3/36132.png&#39;),Path(&#39;train/3/50201.png&#39;),Path(&#39;train/3/39704.png&#39;),Path(&#39;train/3/8475.png&#39;),Path(&#39;train/3/5139.png&#39;),Path(&#39;train/3/32610.png&#39;),Path(&#39;train/3/7784.png&#39;),Path(&#39;train/3/45704.png&#39;)...] . Image.open(images[0]) . a = torch.stack([tensor(Image.open(o)) for o in (path/&#39;train&#39;/&#39;3&#39;).ls()]).reshape(-1, 28*28)/255. b = torch.stack([tensor(Image.open(o)) for o in (path/&#39;train&#39;/&#39;7&#39;).ls()]).reshape(-1, 28*28)/255. X = torch.cat([a, b]) y = tensor([1]*len(a) + [0]*len(b)) a = torch.stack([tensor(Image.open(o)) for o in (path/&#39;valid&#39;/&#39;3&#39;).ls()]).reshape(-1, 28*28)/255. b = torch.stack([tensor(Image.open(o)) for o in (path/&#39;valid&#39;/&#39;7&#39;).ls()]).reshape(-1, 28*28)/255. X_valid = torch.cat([a, b]) y_valid = tensor([1]*len(a) + [0]*len(b)) X.shape, y.shape, X_valid.shape, y_valid.shape . (torch.Size([12396, 784]), torch.Size([12396]), torch.Size([2038, 784]), torch.Size([2038])) . ds = list(zip(X, y)) dl = DataLoader(ds, batch_size=256, shuffle=True) ds_valid = list(zip(X_valid, y_valid)) dl_valid = DataLoader(ds_valid, batch_size=256) dls = DataLoaders(dl, dl_valid) . weights = torch.randn((28*28, 1)).requires_grad_() bias = torch.randn(1).requires_grad_() weights[:3], bias . (tensor([[-0.4108], [ 0.9965], [ 0.5047]], grad_fn=&lt;SliceBackward0&gt;), tensor([-0.0758], requires_grad=True)) . X[:5]@weights + bias . tensor([[ 5.9452], [10.0471], [-8.5174], [ 7.4015], [ 3.0735]], grad_fn=&lt;AddBackward0&gt;) . W = torch.randn((28*28, 1)).requires_grad_() b = torch.randn(1).requires_grad_() lr = 1. X, t = first(dl) def doit(): y = (lambda x: 1/(1+torch.exp(-x)))(X@W + b) loss = ((t-y)**2).mean() loss.backward() print(loss.item(), W.grad.mean(), b.grad) for p in W, b: p.data -= lr*p.grad p.grad.zero_() for _ in range(3): doit() . 0.429675817489624 tensor(-0.0002) tensor([-0.0027]) 0.4291311502456665 tensor(-0.0002) tensor([-0.0027]) 0.42858171463012695 tensor(-0.0002) tensor([-0.0027]) . W = torch.randn((28*28, 1)).requires_grad_() b = torch.randn(1).requires_grad_() lr = 1. def accuracy(): y = (lambda x: 1/(1+torch.exp(-x)))(X_valid@W + b) return ((y &gt; 0.5)==t).float().mean() for epoch in range(30): for X, t in dl: y = (lambda x: 1/(1+torch.exp(-x)))(X@W + b) loss = ((t-y)**2).mean() loss.backward() for p in W, b: p.data -= lr*p.grad p.grad.zero_() print(epoch, loss.item(), accuracy()) . 0 0.42189547419548035 tensor(0.5064) 1 0.43536025285720825 tensor(0.5000) 2 0.40831637382507324 tensor(0.4987) 3 0.4077708125114441 tensor(0.5011) 4 0.4055875837802887 tensor(0.5067) 5 0.40033259987831116 tensor(0.5110) 6 0.39839795231819153 tensor(0.5065) 7 0.38762959837913513 tensor(0.5020) 8 0.36699414253234863 tensor(0.5065) 9 0.39512205123901367 tensor(0.5059) 10 0.3689902424812317 tensor(0.5048) 11 0.34809964895248413 tensor(0.5012) 12 0.34387949109077454 tensor(0.5050) 13 0.3494081497192383 tensor(0.5050) 14 0.3504391312599182 tensor(0.4992) 15 0.333186537027359 tensor(0.5017) 16 0.32519006729125977 tensor(0.5150) 17 0.31659629940986633 tensor(0.5301) 18 0.3194851875305176 tensor(0.5057) 19 0.31758102774620056 tensor(0.5000) 20 0.3440946042537689 tensor(0.5864) 21 0.36963266134262085 tensor(0.5365) 22 0.30330464243888855 tensor(0.5538) 23 0.33537527918815613 tensor(0.4992) 24 0.3367502987384796 tensor(0.5366) 25 0.3061891496181488 tensor(0.4945) 26 0.3117249011993408 tensor(0.5044) 27 0.39691346883773804 tensor(0.5499) 28 0.3109123110771179 tensor(0.5000) 29 0.338569700717926 tensor(0.4946) .",
            "url": "https://doyu.github.io/blog/2022/09/01/a-Deep_Learning_from_scratch.html",
            "relUrl": "/2022/09/01/a-Deep_Learning_from_scratch.html",
            "date": " • Sep 1, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "fastbook 14 Implement ResNet",
            "content": "Modified the one is the book so that it would be easier to read . from fastai.vision.all import * . def get_data(url, presize, resize): path = untar_data(url) dls = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=GrandparentSplitter(valid_name=&#39;val&#39;), get_y=parent_label, item_tfms=Resize(presize), batch_tfms=[ *aug_transforms(min_scale=0.5, size=resize), Normalize.from_stats(*imagenet_stats), ], ).dataloaders(path, bs=128) return dls dls = get_data(URLs.IMAGENETTE_160, 160, 128) . def _conv_block(ni,nf,stride): return nn.Sequential( ConvLayer(ni, nf, stride=stride), ConvLayer(nf, nf, act_cls=None, norm_type=NormType.BatchZero)) . class ResBlock(Module): def __init__(self, ni, nf, stride=1): self.convs = _conv_block(ni,nf,stride) self.idconv = noop if ni==nf else ConvLayer(ni, nf, 1, act_cls=None) self.pool = noop if stride==1 else nn.AvgPool2d(2, ceil_mode=True) def forward(self, x): return F.relu(self.convs(x) + self.idconv(self.pool(x))) . Original &#39;_resnet_stem()&#39; . def _resnet_stem(*sizes): return [ ConvLayer(sizes[i], sizes[i+1], 3, stride = 2 if i==0 else 1) for i in range(len(sizes)-1) ] + [nn.MaxPool2d(kernel_size=3, stride=2, padding=1)] . Modified &#39;_resnet_stem()&#39; . def _resnet_stem(*sizes): # Conv Macro only within this function def __C(i, s): return ConvLayer(sizes[i], sizes[i+1], 3, stride=s) l = [__C(i=0, s=2)] l += [__C(i=i, s=1) for i in range(1, len(sizes)-1)] l += [nn.MaxPool2d(kernel_size=3, stride=2, padding=1)] return l #_resnet_stem(3,32,32,64) . Modified the above &#39;_resnet_stem&#39; to read a little bit easier, where we deal with the 1st iteration and the rest of the others separately since only the 1st time has differenet params. The 1st iter is done out side of the loop and then the loop will do the rest. . Original &#39;ResNet()&#39; . class ResNet(nn.Sequential): def __init__(self, n_out, layers, expansion=1): stem = _resnet_stem(3,32,32,64) self.block_szs = [64, 64, 128, 256, 512] for i in range(1,5): self.block_szs[i] *= expansion blocks = [self._make_layer(*o) for o in enumerate(layers)] super().__init__(*stem, *blocks, nn.AdaptiveAvgPool2d(1), Flatten(), nn.Linear(self.block_szs[-1], n_out)) def _make_layer(self, idx, n_layers): stride = 1 if idx==0 else 2 ch_in,ch_out = self.block_szs[idx:idx+2] return nn.Sequential(*[ ResBlock(ch_in if i==0 else ch_out, ch_out, stride if i==0 else 1) for i in range(n_layers) ]) . Modified &#39;ResNet()&#39; . class ResNet(nn.Sequential): def __init__(self, n_out, layers, expansion=1): self.block_szs = [64, 64, 128, 256, 512] for i in range(1,5): self.block_szs[i] *= expansion l = _resnet_stem(3,32,32,64) l += [self._make_layer(layers[0], *self.block_szs[:2], 1)] # Do 0th l += [self._make_layer(layers[i], *self.block_szs[i:i+2], 2) # Do the rest for i in range(1, len(layers))] # Appending the Head part l += [ nn.AdaptiveAvgPool2d(1), Flatten(), nn.Linear(self.block_szs[-1], n_out) ] super().__init__(*l) def _make_layer(self, n_layers, ch_in, ch_out, stride): l = [ResBlock(ch_in, ch_out, stride)] # Do 0th l += [ResBlock(ch_out, ch_out, 1) # Do the rest for i in range(1, n_layers)] return nn.Sequential(*l) . learn = Learner(dls, ResNet(dls.c, [2,2,2,2]), loss_func=nn.CrossEntropyLoss(), metrics=accuracy).to_fp16() learn.fit_one_cycle(5, 3e-3) . epoch train_loss valid_loss accuracy time . 0 | 1.669369 | 3.107110 | 0.332484 | 00:16 | . 1 | 1.346194 | 1.786952 | 0.458599 | 00:16 | . 2 | 1.105611 | 1.140638 | 0.642293 | 00:16 | . 3 | 0.897768 | 0.917169 | 0.710573 | 00:16 | . 4 | 0.769690 | 0.756307 | 0.761274 | 00:16 | . Bottleneck . dls = get_data(URLs.IMAGENETTE_320, presize=320, resize=224) def _conv_block(ni,nf,stride): return nn.Sequential( ConvLayer(ni, nf//4, 1), ConvLayer(nf//4, nf//4, stride=stride), ConvLayer(nf//4, nf, 1, act_cls=None, norm_type=NormType.BatchZero)) learn = Learner(dls, ResNet(dls.c, [3,4,6,3], 4), loss_func=nn.CrossEntropyLoss(), metrics=accuracy).to_fp16() learn.fit_one_cycle(20, 3e-3) . epoch train_loss valid_loss accuracy time . 0 | 1.609382 | 1.529753 | 0.510318 | 01:30 | . 1 | 1.383831 | 1.579404 | 0.533758 | 01:32 | . 2 | 1.265062 | 1.538380 | 0.535541 | 01:34 | . 3 | 1.160110 | 1.392963 | 0.538344 | 01:34 | . 4 | 1.063608 | 2.391305 | 0.418853 | 01:34 | . 5 | 0.950941 | 1.787218 | 0.471338 | 01:34 | . 6 | 0.848306 | 1.497377 | 0.636433 | 01:34 | . 7 | 0.783319 | 0.930480 | 0.701401 | 01:34 | . 8 | 0.713762 | 1.030928 | 0.681783 | 01:34 | . 9 | 0.639707 | 1.577286 | 0.574013 | 01:34 | . 10 | 0.604874 | 0.995040 | 0.695541 | 01:34 | . 11 | 0.539998 | 0.803806 | 0.754650 | 01:34 | . 12 | 0.484867 | 0.688639 | 0.779618 | 01:34 | . 13 | 0.459248 | 0.712160 | 0.785733 | 01:34 | . 14 | 0.392319 | 0.468867 | 0.857580 | 01:34 | . 15 | 0.353768 | 0.558457 | 0.837197 | 01:34 | . 16 | 0.314284 | 0.418582 | 0.871338 | 01:34 | . 17 | 0.287317 | 0.439048 | 0.866752 | 01:34 | . 18 | 0.260283 | 0.403132 | 0.877452 | 01:34 | . 19 | 0.242763 | 0.425505 | 0.871847 | 01:34 | .",
            "url": "https://doyu.github.io/blog/2022/08/26/fastbook_14_ResNet.html",
            "relUrl": "/2022/08/26/fastbook_14_ResNet.html",
            "date": " • Aug 26, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "fastbook 13 Implement CNN",
            "content": "from fastai.vision.all import * . Try manual convolution for MNIST_SAMPLE! . top_edge = tensor([[-1,-1,-1], [0,0,0], [1,1,1]]).float() . path = untar_data(URLs.MNIST_SAMPLE) Path.BASE_PATH = path path.ls() . (#3) [Path(&#39;train&#39;),Path(&#39;labels.csv&#39;),Path(&#39;valid&#39;)] . im3 = Image.open(path/&#39;train&#39;/&#39;3&#39;/&#39;12.png&#39;) show_image(im3) . im3_t = tensor(im3) im3_t[:3,:3] * top_edge . tensor([[-0., -0., -0.], [0., 0., 0.], [0., 0., 0.]]) . im3_t[:3,:3] * top_edge.sum() . tensor([[0., 0., 0.], [0., 0., 0.], [0., 0., 0.]]) . df = pd.DataFrame(im3_t[:10,:20]) df . 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 . 0 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 1 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 2 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 3 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 4 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 5 0 | 0 | 0 | 12 | 99 | 91 | 142 | 155 | 246 | 182 | 155 | 155 | 155 | 155 | 131 | 52 | 0 | 0 | 0 | 0 | . 6 0 | 0 | 0 | 138 | 254 | 254 | 254 | 254 | 254 | 254 | 254 | 254 | 254 | 254 | 254 | 252 | 210 | 122 | 33 | 0 | . 7 0 | 0 | 0 | 220 | 254 | 254 | 254 | 235 | 189 | 189 | 189 | 189 | 150 | 189 | 205 | 254 | 254 | 254 | 75 | 0 | . 8 0 | 0 | 0 | 35 | 74 | 35 | 35 | 25 | 0 | 0 | 0 | 0 | 0 | 0 | 13 | 224 | 254 | 254 | 153 | 0 | . 9 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 90 | 254 | 254 | 247 | 53 | 0 | . df.style.background_gradient(&#39;Greys&#39;) . &nbsp; 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 . 0 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 1 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 2 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 3 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 4 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 5 0 | 0 | 0 | 12 | 99 | 91 | 142 | 155 | 246 | 182 | 155 | 155 | 155 | 155 | 131 | 52 | 0 | 0 | 0 | 0 | . 6 0 | 0 | 0 | 138 | 254 | 254 | 254 | 254 | 254 | 254 | 254 | 254 | 254 | 254 | 254 | 252 | 210 | 122 | 33 | 0 | . 7 0 | 0 | 0 | 220 | 254 | 254 | 254 | 235 | 189 | 189 | 189 | 189 | 150 | 189 | 205 | 254 | 254 | 254 | 75 | 0 | . 8 0 | 0 | 0 | 35 | 74 | 35 | 35 | 25 | 0 | 0 | 0 | 0 | 0 | 0 | 13 | 224 | 254 | 254 | 153 | 0 | . 9 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 90 | 254 | 254 | 247 | 53 | 0 | . df.style.set_properties(**{&#39;font-size&#39;:&#39;6pt&#39;}).background_gradient(&#39;Greys&#39;) . &nbsp; 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 . 0 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 1 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 2 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 3 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 4 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 5 0 | 0 | 0 | 12 | 99 | 91 | 142 | 155 | 246 | 182 | 155 | 155 | 155 | 155 | 131 | 52 | 0 | 0 | 0 | 0 | . 6 0 | 0 | 0 | 138 | 254 | 254 | 254 | 254 | 254 | 254 | 254 | 254 | 254 | 254 | 254 | 252 | 210 | 122 | 33 | 0 | . 7 0 | 0 | 0 | 220 | 254 | 254 | 254 | 235 | 189 | 189 | 189 | 189 | 150 | 189 | 205 | 254 | 254 | 254 | 75 | 0 | . 8 0 | 0 | 0 | 35 | 74 | 35 | 35 | 25 | 0 | 0 | 0 | 0 | 0 | 0 | 13 | 224 | 254 | 254 | 153 | 0 | . 9 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 90 | 254 | 254 | 247 | 53 | 0 | . (im3_t[4:7,6:9]*top_edge).sum() . tensor(762.) . (im3_t[5:8,6:9]*top_edge).sum() . tensor(135.) . (im3_t[7:10,17:20]*top_edge).sum() . tensor(-29.) . def apply_kernel(row, col, kernel): return (im3_t[row-1:row+2,col-1:col+2]*kernel).sum() . apply_kernel(5,7, top_edge) . tensor(762.) . tmp = [] for i in range(1,5): l = [(i,j) for j in range(1,5)] tmp.append(l) tmp . [[(1, 1), (1, 2), (1, 3), (1, 4)], [(2, 1), (2, 2), (2, 3), (2, 4)], [(3, 1), (3, 2), (3, 3), (3, 4)], [(4, 1), (4, 2), (4, 3), (4, 4)]] . [[(i,j) for j in range(1,5)] for i in range(1,5)] . [[(1, 1), (1, 2), (1, 3), (1, 4)], [(2, 1), (2, 2), (2, 3), (2, 4)], [(3, 1), (3, 2), (3, 3), (3, 4)], [(4, 1), (4, 2), (4, 3), (4, 4)]] . def apply_kernel_(k): &#39;&#39;&#39;&#39;apply specified &#39;kernel&#39; to all 28*28 dimention&#39;&#39;&#39; return tensor([[apply_kernel(i,j,k) for j in range(1,27)] for i in range(1,27)]) show_image(apply_kernel_(top_edge)) . &lt;AxesSubplot:&gt; . left_edge = tensor([[-1,1,0], [-1,1,0], [-1,1,0],]).float() show_image(apply_kernel_(left_edge)) . &lt;AxesSubplot:&gt; . diag1_edge = torch.eye(3).flip(0) diag1_edge[1,0] = -1. diag1_edge[0,1] = -1. diag1_edge . tensor([[ 0., -1., 1.], [-1., 1., 0.], [ 1., 0., 0.]]) . diag2_edge = diag1_edge.flip(1);diag2_edge . tensor([[ 1., -1., 0.], [ 0., 1., -1.], [ 0., 0., 1.]]) . edge_kernels = torch.stack([ left_edge, top_edge, diag1_edge, diag2_edge, ]) edge_kernels.shape . torch.Size([4, 3, 3]) . dls = DataBlock( (ImageBlock(cls=PILImageBW), CategoryBlock), get_items=get_image_files, splitter=GrandparentSplitter(), get_y=parent_label).dataloaders(path) xb, yb = first(dls.valid) xb.shape, yb.shape . (torch.Size([64, 1, 28, 28]), torch.Size([64])) . xb, yb = to_cpu(xb), to_cpu(yb) . kernel shpae is $ ({out _channels}, frac, kH , kW) $ , where ${groups}$ could divide the ${in _channels}$ into # of the ${groups}$ to be masked by the # of ${out _channels}$ respectively. . edge_kernels.shape,edge_kernels.unsqueeze(1).shape,edge_kernels[:,None,:,:].shape . (torch.Size([4, 3, 3]), torch.Size([4, 1, 3, 3]), torch.Size([4, 1, 3, 3])) . edge_kernels = edge_kernels.unsqueeze(1) batch_features = F.conv2d(xb, edge_kernels) batch_features.shape . torch.Size([64, 4, 26, 26]) . show_image(batch_features[0,0]) show_image(batch_features[0,1]) show_image(batch_features[1,0]) show_image(batch_features[1,1]) . &lt;AxesSubplot:&gt; . def new_len(n, ks, stride): return (n + 2*(ks//2) - ks) // stride + 1 # After Conv2d new_len(28, 3, 1) . 28 . simple_net = nn.Sequential( nn.Linear(28*28, 30), nn.ReLU(), nn.Linear(30, 1) ) simple_net . Sequential( (0): Linear(in_features=784, out_features=30, bias=True) (1): ReLU() (2): Linear(in_features=30, out_features=1, bias=True) ) . broken_cnn = nn.Sequential( nn.Conv2d(1, 30, 3, padding=1), nn.ReLU(), nn.Conv2d(30, 1, 3, padding=1) ) broken_cnn . Sequential( (0): Conv2d(1, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU() (2): Conv2d(30, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) ) . broken_cnn(xb).shape . torch.Size([64, 1, 28, 28]) . def conv(ni, nf, ks=3, act=True): res = nn.Conv2d(ni, nf, stride=2, kernel_size=ks, padding=ks//2) if act: res = nn.Sequential(res, nn.ReLU()) return res . simple_cnn = nn.Sequential( conv(1, 4), conv(4, 8), conv(8, 16), conv(16,32), conv(32, 2, act=False), Flatten(), ) Learner(dls, simple_cnn).summary() . Sequential (Input shape: 64 x 1 x 28 x 28) ============================================================================ Layer (type) Output Shape Param # Trainable ============================================================================ 64 x 4 x 14 x 14 Conv2d 40 True ReLU ____________________________________________________________________________ 64 x 8 x 7 x 7 Conv2d 296 True ReLU ____________________________________________________________________________ 64 x 16 x 4 x 4 Conv2d 1168 True ReLU ____________________________________________________________________________ 64 x 32 x 2 x 2 Conv2d 4640 True ReLU ____________________________________________________________________________ 64 x 2 x 1 x 1 Conv2d 578 True ____________________________________________________________________________ 64 x 2 Flatten ____________________________________________________________________________ Total params: 6,722 Total trainable params: 6,722 Total non-trainable params: 0 Optimizer used: &lt;function Adam at 0x7fc89882f040&gt; Loss function: FlattenedLoss of CrossEntropyLoss() Callbacks: - TrainEvalCallback - CastToTensor - Recorder - ProgressCallback . xb, yb = xb.to(&#39;cuda&#39;), yb.to(&#39;cuda&#39;) simple_cnn(xb).shape . torch.Size([64, 2]) . learn = Learner(dls, simple_cnn, loss_func=F.cross_entropy, metrics=accuracy) learn.summary() . Sequential (Input shape: 64 x 1 x 28 x 28) ============================================================================ Layer (type) Output Shape Param # Trainable ============================================================================ 64 x 4 x 14 x 14 Conv2d 40 True ReLU ____________________________________________________________________________ 64 x 8 x 7 x 7 Conv2d 296 True ReLU ____________________________________________________________________________ 64 x 16 x 4 x 4 Conv2d 1168 True ReLU ____________________________________________________________________________ 64 x 32 x 2 x 2 Conv2d 4640 True ReLU ____________________________________________________________________________ 64 x 2 x 1 x 1 Conv2d 578 True ____________________________________________________________________________ 64 x 2 Flatten ____________________________________________________________________________ Total params: 6,722 Total trainable params: 6,722 Total non-trainable params: 0 Optimizer used: &lt;function Adam at 0x7fc89882f040&gt; Loss function: &lt;function cross_entropy at 0x7fc8bead5ca0&gt; Callbacks: - TrainEvalCallback - CastToTensor - Recorder - ProgressCallback . learn.fit_one_cycle(2, 0.01) . epoch train_loss valid_loss accuracy time . 0 | 0.056674 | 0.038932 | 0.988714 | 00:07 | . 1 | 0.022213 | 0.023022 | 0.992149 | 00:03 | . m = learn.model[0] m . Sequential( (0): Conv2d(1, 4, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)) (1): ReLU() ) . m[0].weight.shape, m[0].bias.shape, . (torch.Size([4, 1, 3, 3]), torch.Size([4])) . path = untar_data(URLs.MNIST) path.ls() . (#2) [Path(&#39;/root/.fastai/data/mnist_png/training&#39;),Path(&#39;/root/.fastai/data/mnist_png/testing&#39;)] . def get_dls(bs=64): return DataBlock( blocks=(ImageBlock(cls=PILImageBW), CategoryBlock), get_items=get_image_files, splitter=GrandparentSplitter(&#39;training&#39;, &#39;testing&#39;), get_y=parent_label, batch_tfms=Normalize(), ).dataloaders(path, bs=bs) dls = get_dls() dls.show_batch(figsize=(4,4)) . def simple_cnn(): return nn.Sequential( conv(1, 8, ks=5), conv(8, 16), conv(16,32), conv(32,64), conv(64, 10, act=False), Flatten(), ) Learner(dls, simple_cnn()).summary() . Sequential (Input shape: 64 x 1 x 28 x 28) ============================================================================ Layer (type) Output Shape Param # Trainable ============================================================================ 64 x 8 x 14 x 14 Conv2d 208 True ReLU ____________________________________________________________________________ 64 x 16 x 7 x 7 Conv2d 1168 True ReLU ____________________________________________________________________________ 64 x 32 x 4 x 4 Conv2d 4640 True ReLU ____________________________________________________________________________ 64 x 64 x 2 x 2 Conv2d 18496 True ReLU ____________________________________________________________________________ 64 x 10 x 1 x 1 Conv2d 5770 True ____________________________________________________________________________ 64 x 10 Flatten ____________________________________________________________________________ Total params: 30,282 Total trainable params: 30,282 Total non-trainable params: 0 Optimizer used: &lt;function Adam at 0x7fc89882f040&gt; Loss function: FlattenedLoss of CrossEntropyLoss() Callbacks: - TrainEvalCallback - CastToTensor - Recorder - ProgressCallback . from fastai.callback.hook import * def fit(epochs=1): learn = Learner(dls, simple_cnn(), loss_func=F.cross_entropy, metrics=accuracy, cbs=ActivationStats(with_hist=True)) learn.fit(epochs, 0.06) return learn learn = fit() . /root/mambaforge/lib/python3.9/site-packages/fastai/callback/core.py:72: UserWarning: You are shadowing an attribute (modules) that exists in the learner. Use `self.learn.modules` to avoid this warn(f&#34;You are shadowing an attribute ({name}) that exists in the learner. Use `self.learn.{name}` to avoid this&#34;) . epoch train_loss valid_loss accuracy time . 0 | 2.306356 | 2.308944 | 0.100900 | 01:06 | . learn.activation_stats.plot_layer_stats(0) . learn.activation_stats.plot_layer_stats(-2) . dls = get_dls(512) learn = fit() . epoch train_loss valid_loss accuracy time . 0 | 0.557525 | 0.320446 | 0.896600 | 00:49 | . learn.activation_stats.plot_layer_stats(-2) . from fastai.callback.hook import * def fit(epochs=1): learn = Learner(dls, simple_cnn(), loss_func=F.cross_entropy, metrics=accuracy, cbs=ActivationStats(with_hist=True)) learn.fit_one_cycle(epochs, 0.06) return learn learn = fit() . epoch train_loss valid_loss accuracy time . 0 | 0.220707 | 0.074086 | 0.976800 | 00:41 | . learn.recorder.plot_sched() . learn.activation_stats.plot_layer_stats(-2) . learn.activation_stats.color_dim(-2) . def conv(ni, nf, ks=3, act=True): res = nn.Conv2d(ni, nf, stride=2, kernel_size=ks, padding=ks//2) res = nn.Sequential(res, nn.BatchNorm2d(nf)) if act: res = nn.Sequential(res, nn.ReLU()) return res learn = fit() . epoch train_loss valid_loss accuracy time . 0 | 0.136115 | 0.058227 | 0.985200 | 00:47 | . learn.activation_stats.color_dim(-2) . def fit(epochs=1, lr=0.06): learn = Learner(dls, simple_cnn(), loss_func=F.cross_entropy, metrics=accuracy, cbs=ActivationStats(with_hist=True)) learn.fit_one_cycle(epochs, 0.06) return learn learn = fit(5, lr=0.1) learn = fit(5, lr=0.1) . epoch train_loss valid_loss accuracy time . 0 | 0.216674 | 0.145344 | 0.955200 | 00:55 | . 1 | 0.082331 | 0.063296 | 0.981300 | 00:48 | . 2 | 0.048664 | 0.046084 | 0.985700 | 00:47 | . 3 | 0.031943 | 0.029971 | 0.990000 | 00:46 | . 4 | 0.015729 | 0.025128 | 0.992100 | 00:52 | . epoch train_loss valid_loss accuracy time . 0 | 0.213008 | 0.103855 | 0.967300 | 00:45 | . 1 | 0.080208 | 0.058328 | 0.981600 | 00:50 | . 2 | 0.050969 | 0.041169 | 0.987600 | 00:53 | . 3 | 0.032378 | 0.029437 | 0.991200 | 00:46 | . 4 | 0.016578 | 0.023753 | 0.992100 | 00:47 | .",
            "url": "https://doyu.github.io/blog/2022/08/26/fastbook_13_Implement_CNN.html",
            "relUrl": "/2022/08/26/fastbook_13_Implement_CNN.html",
            "date": " • Aug 26, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "fastbook 09 (DT, RF & DL) with California Housing Prices",
            "content": "from fastai.imports import * from fastai.tabular.all import * from kaggle import api . data = &#39;camnugent/california-housing-prices&#39; api.dataset_list_cli(search=data) . ref title size lastUpdated downloadCount voteCount usabilityRating -- - -- - - camnugent/california-housing-prices California Housing Prices 400KB 2017-11-24 03:14:59 70928 741 0.85294116 . path = Path(data.split(&#39;/&#39;)[1]) api.dataset_download_files(data, path) path.ls() . (#2) [Path(&#39;california-housing-prices/california-housing-prices.zip&#39;),Path(&#39;california-housing-prices/housing.csv&#39;)] . import zipfile zipfile.ZipFile(path.ls()[0]).extractall(path) path.ls() . (#2) [Path(&#39;california-housing-prices/california-housing-prices.zip&#39;),Path(&#39;california-housing-prices/housing.csv&#39;)] . df = pd.read_csv(path/&#39;housing.csv&#39;, low_memory=False) df.head() . longitude latitude housing_median_age total_rooms total_bedrooms population households median_income median_house_value ocean_proximity . 0 -122.23 | 37.88 | 41.0 | 880.0 | 129.0 | 322.0 | 126.0 | 8.3252 | 452600.0 | NEAR BAY | . 1 -122.22 | 37.86 | 21.0 | 7099.0 | 1106.0 | 2401.0 | 1138.0 | 8.3014 | 358500.0 | NEAR BAY | . 2 -122.24 | 37.85 | 52.0 | 1467.0 | 190.0 | 496.0 | 177.0 | 7.2574 | 352100.0 | NEAR BAY | . 3 -122.25 | 37.85 | 52.0 | 1274.0 | 235.0 | 558.0 | 219.0 | 5.6431 | 341300.0 | NEAR BAY | . 4 -122.25 | 37.85 | 52.0 | 1627.0 | 280.0 | 565.0 | 259.0 | 3.8462 | 342200.0 | NEAR BAY | . df.columns . Index([&#39;longitude&#39;, &#39;latitude&#39;, &#39;housing_median_age&#39;, &#39;total_rooms&#39;, &#39;total_bedrooms&#39;, &#39;population&#39;, &#39;households&#39;, &#39;median_income&#39;, &#39;median_house_value&#39;, &#39;ocean_proximity&#39;], dtype=&#39;object&#39;) . df.hist(figsize=(12,12)) . array([[&lt;AxesSubplot:title={&#39;center&#39;:&#39;longitude&#39;}&gt;, &lt;AxesSubplot:title={&#39;center&#39;:&#39;latitude&#39;}&gt;, &lt;AxesSubplot:title={&#39;center&#39;:&#39;housing_median_age&#39;}&gt;], [&lt;AxesSubplot:title={&#39;center&#39;:&#39;total_rooms&#39;}&gt;, &lt;AxesSubplot:title={&#39;center&#39;:&#39;total_bedrooms&#39;}&gt;, &lt;AxesSubplot:title={&#39;center&#39;:&#39;population&#39;}&gt;], [&lt;AxesSubplot:title={&#39;center&#39;:&#39;households&#39;}&gt;, &lt;AxesSubplot:title={&#39;center&#39;:&#39;median_income&#39;}&gt;, &lt;AxesSubplot:title={&#39;center&#39;:&#39;median_house_value&#39;}&gt;]], dtype=object) . x = &quot;total_rooms total_bedrooms population households&quot;.split() df[x] = np.log(df[x]) df.hist(figsize=(12,12)) . array([[&lt;AxesSubplot:title={&#39;center&#39;:&#39;longitude&#39;}&gt;, &lt;AxesSubplot:title={&#39;center&#39;:&#39;latitude&#39;}&gt;, &lt;AxesSubplot:title={&#39;center&#39;:&#39;housing_median_age&#39;}&gt;], [&lt;AxesSubplot:title={&#39;center&#39;:&#39;total_rooms&#39;}&gt;, &lt;AxesSubplot:title={&#39;center&#39;:&#39;total_bedrooms&#39;}&gt;, &lt;AxesSubplot:title={&#39;center&#39;:&#39;population&#39;}&gt;], [&lt;AxesSubplot:title={&#39;center&#39;:&#39;households&#39;}&gt;, &lt;AxesSubplot:title={&#39;center&#39;:&#39;median_income&#39;}&gt;, &lt;AxesSubplot:title={&#39;center&#39;:&#39;median_house_value&#39;}&gt;]], dtype=object) . cat = [&#39;ocean_proximity&#39;] cont = [&#39;longitude&#39;, &#39;latitude&#39;, &#39;housing_median_age&#39;, &#39;total_rooms&#39;, &#39;total_bedrooms&#39;, &#39;population&#39;, &#39;households&#39;, &#39;median_income&#39;] procs = [Categorify, FillMissing, Normalize] to = TabularPandas(df, procs, cat, cont, &#39;median_house_value&#39;, RegressionBlock(), RandomSplitter()(df), reduce_memory=False).dataloaders(path=&#39;.&#39;) . xs,y = to.train.xs,to.train.y val_xs,val_y = to.valid.xs,to.valid.y . RF . from sklearn.ensemble import RandomForestRegressor m = RandomForestRegressor(100, min_samples_leaf=5).fit(xs, y) print(&#39;MAE:&#39;, abs(val_y - m.predict(val_xs)).mean()) x = pd.DataFrame({&#39;cols&#39;:xs.columns, &#39;imp&#39;:m.feature_importances_}).sort_values(&#39;imp&#39;, ascending=False) x.set_index(&#39;cols&#39;).plot(kind=&#39;barh&#39;) x . MAE: 32150.47857078855 . cols imp . 9 median_income | 0.551278 | . 0 ocean_proximity | 0.117905 | . 2 longitude | 0.108562 | . 3 latitude | 0.106565 | . 4 housing_median_age | 0.046819 | . 7 population | 0.023806 | . 5 total_rooms | 0.017090 | . 6 total_bedrooms | 0.014583 | . 8 households | 0.013367 | . 1 total_bedrooms_na | 0.000025 | . DT . from sklearn.tree import DecisionTreeRegressor, export_graphviz m = DecisionTreeRegressor(max_leaf_nodes=30).fit(xs, y) preds = m.predict(val_xs) abs(val_y - preds).mean() . 48784.69675117493 . import graphviz def draw_tree(t, df, size=10, ratio=0.6, precision=2, **kwargs): s=export_graphviz(t, out_file=None, feature_names=df.columns, filled=True, rounded=True, special_characters=True, rotate=False, precision=precision, **kwargs) return graphviz.Source(re.sub(&#39;Tree {&#39;, f&#39;Tree {{ size={size}; ratio={ratio}&#39;, s)) draw_tree(m, xs, size=10) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; Tree 0 median_income ≤ 0.6 squared_error = 13450096298.46 samples = 16512 value = 207451.23 1 median_income ≤ &#45;0.39 squared_error = 8407931988.56 samples = 12979 value = 173549.01 0&#45;&gt;1 True 2 median_income ≤ 1.56 squared_error = 12239470987.85 samples = 3533 value = 331996.07 0&#45;&gt;2 False 3 ocean_proximity ≤ 1.5 squared_error = 5608496426.91 samples = 6508 value = 136635.1 1&#45;&gt;3 4 ocean_proximity ≤ 2.5 squared_error = 8474682591.94 samples = 6471 value = 210673.98 1&#45;&gt;4 11 longitude ≤ 0.63 squared_error = 5305068924.25 samples = 2272 value = 172086.5 3&#45;&gt;11 12 ocean_proximity ≤ 2.5 squared_error = 4735596941.86 samples = 4236 value = 117620.57 3&#45;&gt;12 33 latitude ≤ &#45;0.35 squared_error = 8027839253.15 samples = 838 value = 199719.35 11&#45;&gt;33 34 squared_error = 3006960893.71 samples = 1434 value = 155938.43 11&#45;&gt;34 41 squared_error = 8932852306.54 samples = 516 value = 224786.46 33&#45;&gt;41 42 squared_error = 3957031571.03 samples = 322 value = 159549.69 33&#45;&gt;42 13 squared_error = 1618563072.66 samples = 2862 value = 92402.73 12&#45;&gt;13 14 longitude ≤ 1.21 squared_error = 7144451228.7 samples = 1374 value = 170148.55 12&#45;&gt;14 43 latitude ≤ 1.06 squared_error = 7932045122.76 samples = 1104 value = 181822.48 14&#45;&gt;43 44 squared_error = 1088357213.85 samples = 270 value = 122415.19 14&#45;&gt;44 45 longitude ≤ &#45;1.37 squared_error = 8053807525.29 samples = 893 value = 197257.01 43&#45;&gt;45 46 squared_error = 2141470900.47 samples = 211 value = 116500.0 43&#45;&gt;46 47 squared_error = 10674715505.32 samples = 136 value = 283391.22 45&#45;&gt;47 48 latitude ≤ 0.91 squared_error = 6010591719.13 samples = 757 value = 181782.44 45&#45;&gt;48 53 squared_error = 6044835831.42 samples = 411 value = 206049.16 48&#45;&gt;53 54 squared_error = 4439504845.45 samples = 346 value = 152956.94 48&#45;&gt;54 7 ocean_proximity ≤ 1.5 squared_error = 6944833238.75 samples = 4829 value = 195279.97 4&#45;&gt;7 8 longitude ≤ &#45;1.38 squared_error = 10227315294.42 samples = 1642 value = 255946.63 4&#45;&gt;8 9 longitude ≤ 0.63 squared_error = 6603157420.73 samples = 2999 value = 227343.2 7&#45;&gt;9 10 squared_error = 3059009268.71 samples = 1830 value = 142734.81 7&#45;&gt;10 19 latitude ≤ &#45;0.68 squared_error = 8577116546.33 samples = 1371 value = 255556.79 9&#45;&gt;19 20 squared_error = 3705941264.15 samples = 1628 value = 203583.49 9&#45;&gt;20 21 longitude ≤ 0.6 squared_error = 10906702197.27 samples = 485 value = 315459.68 19&#45;&gt;21 22 squared_error = 4262355206.19 samples = 886 value = 222765.7 19&#45;&gt;22 31 squared_error = 7005312830.96 samples = 223 value = 366057.56 21&#45;&gt;31 32 latitude ≤ &#45;0.74 squared_error = 10193604375.28 samples = 262 value = 272393.55 21&#45;&gt;32 35 squared_error = 5039079358.57 samples = 169 value = 228079.88 32&#45;&gt;35 36 squared_error = 9507385666.05 samples = 93 value = 352920.55 32&#45;&gt;36 17 housing_median_age ≤ 1.49 squared_error = 10110054299.53 samples = 456 value = 316712.38 8&#45;&gt;17 18 longitude ≤ 1.21 squared_error = 8306840619.78 samples = 1186 value = 232583.07 8&#45;&gt;18 39 squared_error = 7570491263.42 samples = 249 value = 276532.15 17&#45;&gt;39 40 latitude ≤ 0.99 squared_error = 8886817723.63 samples = 207 value = 365045.11 17&#45;&gt;40 51 squared_error = 4345060048.08 samples = 110 value = 312891.85 40&#45;&gt;51 52 squared_error = 7454890227.25 samples = 97 value = 424187.99 40&#45;&gt;52 27 latitude ≤ 0.91 squared_error = 8177004850.63 samples = 1041 value = 244176.97 18&#45;&gt;27 28 squared_error = 1345679042.09 samples = 145 value = 149346.9 18&#45;&gt;28 29 squared_error = 9130254958.69 samples = 633 value = 276095.3 27&#45;&gt;29 30 squared_error = 2665192995.42 samples = 408 value = 194656.62 27&#45;&gt;30 5 housing_median_age ≤ &#45;0.1 squared_error = 8946122397.82 samples = 2483 value = 292783.99 2&#45;&gt;5 6 median_income ≤ 2.05 squared_error = 7793088915.86 samples = 1050 value = 424723.3 2&#45;&gt;6 15 median_income ≤ 0.95 squared_error = 6509956975.94 samples = 1472 value = 266567.96 5&#45;&gt;15 16 median_income ≤ 1.03 squared_error = 10035513797.35 samples = 1011 value = 330954.11 5&#45;&gt;16 37 squared_error = 5814008906.84 samples = 712 value = 240696.23 15&#45;&gt;37 38 squared_error = 5947410362.83 samples = 760 value = 290805.7 15&#45;&gt;38 25 squared_error = 9187757434.12 samples = 642 value = 304493.84 16&#45;&gt;25 26 squared_error = 8172966970.11 samples = 369 value = 376990.67 16&#45;&gt;26 23 housing_median_age ≤ &#45;0.17 squared_error = 7769016670.28 samples = 408 value = 373146.5 6&#45;&gt;23 24 households ≤ &#45;3.67 squared_error = 5043430957.38 samples = 642 value = 457501.09 6&#45;&gt;24 49 squared_error = 5688658380.81 samples = 244 value = 339658.68 23&#45;&gt;49 50 squared_error = 6713344468.47 samples = 164 value = 422969.84 23&#45;&gt;50 55 squared_error = 17028586432.78 samples = 14 value = 271057.29 24&#45;&gt;55 56 median_income ≤ 2.7 squared_error = 3984037152.54 samples = 628 value = 461657.48 24&#45;&gt;56 57 squared_error = 5361175172.58 samples = 310 value = 432276.53 56&#45;&gt;57 58 squared_error = 979667506.0 samples = 318 value = 490299.28 56&#45;&gt;58 DNN . learn = tabular_learner(to, metrics=L1LossFlat(), layers=[10,10]) learn.lr_find(suggest_funcs=(slide, valley)) . SuggestedLRs(slide=6.309573450380412e-07, valley=0.17378008365631104) . learn.fit(10, lr=0.1) learn.recorder.plot_loss() . epoch train_loss valid_loss None time . 0 | 52540518400.000000 | 49501884416.000000 | 195610.734375 | 00:02 | . 1 | 41421684736.000000 | 36557541376.000000 | 169839.937500 | 00:02 | . 2 | 30079684608.000000 | 28439451648.000000 | 151214.703125 | 00:02 | . 3 | 20997189632.000000 | 15527985152.000000 | 108336.179688 | 00:02 | . 4 | 14592946176.000000 | 18044657664.000000 | 120398.703125 | 00:02 | . 5 | 10348133376.000000 | 7776253952.000000 | 70381.062500 | 00:02 | . 6 | 7479977984.000000 | 6708659200.000000 | 64639.753906 | 00:02 | . 7 | 5428857344.000000 | 4700456960.000000 | 49034.535156 | 00:02 | . 8 | 4591148032.000000 | 3880753408.000000 | 42761.687500 | 00:02 | . 9 | 4247306496.000000 | 3972862464.000000 | 43510.292969 | 00:02 | .",
            "url": "https://doyu.github.io/blog/2022/08/19/california_housing_RF_DL.html",
            "relUrl": "/2022/08/19/california_housing_RF_DL.html",
            "date": " • Aug 19, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "Tabular data",
            "content": "! [ -e /content ] &amp;&amp; pip install -Uqq fastbook kaggle waterfallcharts treeinterpreter dtreeviz import fastbook fastbook.setup_book() from fastbook import * from pandas.api.types import is_string_dtype, is_numeric_dtype, is_categorical_dtype from fastai.tabular.all import * from sklearn.ensemble import RandomForestRegressor from sklearn.tree import DecisionTreeRegressor from dtreeviz.trees import * from IPython.display import Image, display_svg, SVG pd.options.display.max_rows = 20 pd.options.display.max_columns = 8 . path = Path(&#39;/root/.fastai/archive/bluebook-for-bulldozers&#39;) Path.BASE_PATH = path path.ls(file_type=&#39;text&#39;) . (#7) [Path(&#39;Machine_Appendix.csv&#39;),Path(&#39;TrainAndValid.csv&#39;),Path(&#39;random_forest_benchmark_test.csv&#39;),Path(&#39;Test.csv&#39;),Path(&#39;median_benchmark.csv&#39;),Path(&#39;ValidSolution.csv&#39;),Path(&#39;Valid.csv&#39;)] . df = pd.read_csv(path/&#39;TrainAndValid.csv&#39;, low_memory=False) df.columns . Index([&#39;SalesID&#39;, &#39;SalePrice&#39;, &#39;MachineID&#39;, &#39;ModelID&#39;, &#39;datasource&#39;, &#39;auctioneerID&#39;, &#39;YearMade&#39;, &#39;MachineHoursCurrentMeter&#39;, &#39;UsageBand&#39;, &#39;saledate&#39;, &#39;fiModelDesc&#39;, &#39;fiBaseModel&#39;, &#39;fiSecondaryDesc&#39;, &#39;fiModelSeries&#39;, &#39;fiModelDescriptor&#39;, &#39;ProductSize&#39;, &#39;fiProductClassDesc&#39;, &#39;state&#39;, &#39;ProductGroup&#39;, &#39;ProductGroupDesc&#39;, &#39;Drive_System&#39;, &#39;Enclosure&#39;, &#39;Forks&#39;, &#39;Pad_Type&#39;, &#39;Ride_Control&#39;, &#39;Stick&#39;, &#39;Transmission&#39;, &#39;Turbocharged&#39;, &#39;Blade_Extension&#39;, &#39;Blade_Width&#39;, &#39;Enclosure_Type&#39;, &#39;Engine_Horsepower&#39;, &#39;Hydraulics&#39;, &#39;Pushblock&#39;, &#39;Ripper&#39;, &#39;Scarifier&#39;, &#39;Tip_Control&#39;, &#39;Tire_Size&#39;, &#39;Coupler&#39;, &#39;Coupler_System&#39;, &#39;Grouser_Tracks&#39;, &#39;Hydraulics_Flow&#39;, &#39;Track_Type&#39;, &#39;Undercarriage_Pad_Width&#39;, &#39;Stick_Length&#39;, &#39;Thumb&#39;, &#39;Pattern_Changer&#39;, &#39;Grouser_Type&#39;, &#39;Backhoe_Mounting&#39;, &#39;Blade_Type&#39;, &#39;Travel_Controls&#39;, &#39;Differential_Type&#39;, &#39;Steering_Controls&#39;], dtype=&#39;object&#39;) . str(df.ProductSize.unique()[1:]) . &#34;[&#39;Medium&#39; &#39;Small&#39; &#39;Large / Medium&#39; &#39;Mini&#39; &#39;Large&#39; &#39;Compact&#39;]&#34; . sizes = &#39;Large&#39;, &#39;Large / Medium&#39;, &#39;Medium&#39;, &#39;Small&#39;, &#39;Mini&#39;, &#39;Compact&#39; df.ProductSize = df.ProductSize.astype(&#39;category&#39;) df.ProductSize.cat.set_categories(sizes, ordered=True, inplace=True) df.ProductSize . /root/mambaforge/lib/python3.9/site-packages/pandas/core/arrays/categorical.py:2747: FutureWarning: The `inplace` parameter in pandas.Categorical.set_categories is deprecated and will be removed in a future version. Removing unused categories will always return a new Categorical object. res = method(*args, **kwargs) . 0 NaN 1 Medium 2 NaN 3 Small 4 NaN ... 412693 Mini 412694 Mini 412695 Mini 412696 Mini 412697 Mini Name: ProductSize, Length: 412698, dtype: category Categories (6, object): [&#39;Large&#39; &lt; &#39;Large / Medium&#39; &lt; &#39;Medium&#39; &lt; &#39;Small&#39; &lt; &#39;Mini&#39; &lt; &#39;Compact&#39;] . dep_var = &#39;SalePrice&#39; df[dep_var] = np.log(df[dep_var]) . len(df.columns) . 53 . df = add_datepart(df, &#39;saledate&#39;) len(df.columns) . 65 . &#39; &#39;.join(o for o in df.columns if o.startswith(&#39;sale&#39;)) . &#39;saleYear saleMonth saleWeek saleDay saleDayofweek saleDayofyear saleIs_month_end saleIs_month_start saleIs_quarter_end saleIs_quarter_start saleIs_year_end saleIs_year_start saleElapsed&#39; . procs = [Categorify, FillMissing] . cond = (df.saleYear&lt;2011) | (df.saleMonth&lt;10) train_idx = np.where(cond)[0] valid_idx = np.where(~cond)[0] splits = (train_idx.tolist(), valid_idx.tolist()) #splits . cont, cat = cont_cat_split(df, 1, dep_var=dep_var) . to = TabularPandas(df, procs, cat, cont, y_names=dep_var, splits=splits) to.classes[&quot;ProductSize&quot;] . [&#39;#na#&#39;, &#39;Large&#39;, &#39;Large / Medium&#39;, &#39;Medium&#39;, &#39;Small&#39;, &#39;Mini&#39;, &#39;Compact&#39;] . save_pickle(path/&#39;to.pkl&#39;, to) . to = load_pickle(path/&#39;to.pkl&#39;) . xs, y = to.train.xs, to.train.y valid_xs, valid_y = to.valid.xs, to.valid.y . m = DecisionTreeRegressor(max_leaf_nodes=4) m.fit(xs, y) draw_tree(m, xs, size=10, leaves_parallel=True, precision=2) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; Tree 0 Coupler_System ≤ 0.5 squared_error = 0.48 samples = 404710 value = 10.1 1 YearMade ≤ 1991.5 squared_error = 0.42 samples = 360847 value = 10.21 0&#45;&gt;1 True 2 squared_error = 0.12 samples = 43863 value = 9.21 0&#45;&gt;2 False 3 squared_error = 0.37 samples = 155724 value = 9.97 1&#45;&gt;3 4 ProductSize ≤ 4.5 squared_error = 0.37 samples = 205123 value = 10.4 1&#45;&gt;4 5 squared_error = 0.31 samples = 182403 value = 10.5 4&#45;&gt;5 6 squared_error = 0.17 samples = 22720 value = 9.62 4&#45;&gt;6 xs.loc[xs.YearMade&lt;1900, &#39;YearMade&#39;] = 1950 valid_xs.loc[valid_xs.YearMade&lt;1900, &#39;YearMade&#39;] = 1950 . def r_mse(pred,y): return round(math.sqrt(((pred-y)**2).mean()), 6) def m_rmse(m, xs, y): return r_mse(m.predict(xs), y) . m = DecisionTreeRegressor(min_samples_leaf=25).fit(to.train.xs, to.train.y) m_rmse(m, xs, y), m_rmse(m, valid_xs, valid_y), m.get_n_leaves() . (0.248595, 0.323441, 12397) . def rf(xs, y, n_estimators=40, max_samples=200_000, max_features=0.5, min_samples_leaf=5, **kwargs): m = RandomForestRegressor(n_jobs=-1, n_estimators=n_estimators, max_samples=max_samples, max_features=max_features, min_samples_leaf=min_samples_leaf, oob_score=True).fit(xs, y) return m . m = rf(xs, y) . m_rmse(m, xs, y), m_rmse(m, valid_xs, valid_y) . (0.170917, 0.232626) . def rf_feat_importance(m, df): df_ret = pd.DataFrame({&#39;cols&#39;:df.columns, &#39;imp&#39;:m.feature_importances_} ).sort_values(&#39;imp&#39;, ascending=False) return df_ret . fi = rf_feat_importance(m, xs) fi[:10] . cols imp . 57 YearMade | 0.173388 | . 6 ProductSize | 0.110295 | . 30 Coupler_System | 0.100494 | . 7 fiProductClassDesc | 0.069326 | . 54 ModelID | 0.055765 | . 65 saleElapsed | 0.050687 | . 32 Hydraulics_Flow | 0.048636 | . 3 fiSecondaryDesc | 0.047010 | . 31 Grouser_Tracks | 0.044165 | . 1 fiModelDesc | 0.031284 | . def plot_fi(fi): return fi.plot(&#39;cols&#39;, &#39;imp&#39;, &#39;barh&#39;, figsize=(12, 7), legend=False) plot_fi(fi[:30]) . &lt;AxesSubplot:ylabel=&#39;cols&#39;&gt; . to_keep = fi[fi.imp&gt;0.005].cols len(to_keep) . 22 . xs_imp = xs[to_keep] valid_xs_imp = valid_xs[to_keep] . m = rf(xs_imp, y) m_rmse(m, xs_imp, y), m_rmse(m, valid_xs_imp, valid_y), . (0.180775, 0.23147) . plot_fi(rf_feat_importance(m, xs_imp)) . &lt;AxesSubplot:ylabel=&#39;cols&#39;&gt; . cluster_columns(xs_imp) . def get_oob(df): m = RandomForestRegressor(n_estimators=40, min_samples_leaf=15, max_samples=50000, max_features=0.5, n_jobs=-1, oob_score=True) m.fit(df, y) return m.oob_score_ get_oob(xs_imp) . 0.8778685460797769 . {c:get_oob(xs_imp.drop(c, axis=1)) for c in ( &#39;saleYear&#39;, &#39;saleElapsed&#39;)}#, &#39;ProductGroupDesc&#39;,&#39;ProductGroup&#39;, # &#39;fiModelDesc&#39;, &#39;fiBaseModel&#39;, # &#39;Hydraulics_Flow&#39;,&#39;Grouser_Tracks&#39;, &#39;Coupler_System&#39;)} . {&#39;saleYear&#39;: 0.8761478210481896, &#39;saleElapsed&#39;: 0.872101137022772} . to_drop = [&#39;saleYear&#39;, &#39;ProductGroupDesc&#39;, &#39;fiBaseModel&#39;, &#39;Grouser_Tracks&#39;] get_oob(xs_imp.drop(to_drop, axis=1)) . 0.874826620033826 . xs_final = xs_imp.drop(to_drop, axis=1) valid_xs_final = valid_xs_imp.drop(to_drop, axis=1) m = rf(xs_final, y) m_rmse(m, xs_final, y), m_rmse(m, valid_xs_final, valid_y) . (0.182696, 0.233411) . p = valid_xs_final[&#39;ProductSize&#39;].value_counts(sort=False).plot.barh() c = to.classes[&#39;ProductSize&#39;] plt.yticks(range(len(c)), c); . ax = valid_xs_final[&#39;YearMade&#39;].hist() . from sklearn.inspection import plot_partial_dependence fig,ax = plt.subplots(figsize=(12, 4)) plot_partial_dependence(m, valid_xs_final, [&#39;YearMade&#39;,&#39;ProductSize&#39;], grid_resolution=20, ax=ax); . /root/mambaforge/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function plot_partial_dependence is deprecated; Function `plot_partial_dependence` is deprecated in 1.0 and will be removed in 1.2. Use PartialDependenceDisplay.from_estimator instead warnings.warn(msg, category=FutureWarning) . !pip install -Uqq --user treeinterpreter . !pip install -Uqq --user waterfallcharts . import warnings warnings.simplefilter(&#39;ignore&#39;, FutureWarning) from treeinterpreter import treeinterpreter from waterfall_chart import plot as waterfall . row = valid_xs_final.iloc[:5] #row.values.shape . prediction, bias, contributions = treeinterpreter.predict(m, row.values) . prediction[0], bias[0], contributions[0].sum() . (array([9.95946805]), 10.104300126903656, -0.14483207244735555) . waterfall(valid_xs_final.columns, contributions[0], threshold=0.08, rotation_value=45,formatting=&#39;{:,.3f}&#39;); . time_vars = [&#39;SalesID&#39;,&#39;MachineID&#39;] xs_final_time = xs_final.drop(time_vars, axis=1) valid_xs_time = valid_xs_final.drop(time_vars, axis=1) . df_nn = pd.read_csv(path/&#39;TrainAndValid.csv&#39;, low_memory=False) df_nn[&#39;ProductSize&#39;] = df_nn[&#39;ProductSize&#39;].astype(&#39;category&#39;) df_nn[&#39;ProductSize&#39;].cat.set_categories(sizes, ordered=True, inplace=True) df_nn[dep_var] = np.log(df_nn[dep_var]) df_nn = add_datepart(df_nn, &#39;saledate&#39;) . df_nn_final = df_nn[list(xs_final_time.columns) + [dep_var]] . df_nn_final.columns . Index([&#39;YearMade&#39;, &#39;ProductSize&#39;, &#39;Coupler_System&#39;, &#39;fiProductClassDesc&#39;, &#39;ModelID&#39;, &#39;saleElapsed&#39;, &#39;Hydraulics_Flow&#39;, &#39;fiSecondaryDesc&#39;, &#39;fiModelDesc&#39;, &#39;ProductGroup&#39;, &#39;Enclosure&#39;, &#39;fiModelDescriptor&#39;, &#39;Hydraulics&#39;, &#39;Drive_System&#39;, &#39;Tire_Size&#39;, &#39;Pad_Type&#39;, &#39;SalePrice&#39;], dtype=&#39;object&#39;) . cont_nn,cat_nn = cont_cat_split(df_nn_final, max_card=9000, dep_var=dep_var) . cont_nn . [&#39;saleElapsed&#39;] . df_nn_final[cat_nn].nunique() . YearMade 73 ProductSize 6 Coupler_System 2 fiProductClassDesc 74 ModelID 5281 Hydraulics_Flow 3 fiSecondaryDesc 177 fiModelDesc 5059 ProductGroup 6 Enclosure 6 fiModelDescriptor 140 Hydraulics 12 Drive_System 4 Tire_Size 17 Pad_Type 4 dtype: int64 . cat_nn.remove(&#39;fiModelDescriptor&#39;) . procs_nn = [Categorify, FillMissing, Normalize] to_nn = TabularPandas(df_nn_final, procs_nn, cat_nn, cont_nn, splits=splits, y_names=dep_var) dls = to_nn.dataloaders(1024) y = to_nn.train.y y.min(),y.max() . (8.465899, 11.863583) . learn = tabular_learner(dls, y_range=(8,12), layers=[500,250], n_out=1, loss_func=F.mse_loss) learn.fit_one_cycle(5, 1e-2) preds,targs = learn.get_preds() r_mse(preds,targs) . epoch train_loss valid_loss time . 0 | 0.062922 | 0.062253 | 00:04 | . 1 | 0.053299 | 0.058312 | 00:04 | . 2 | 0.047690 | 0.053533 | 00:04 | . 3 | 0.043369 | 0.052020 | 00:04 | . 4 | 0.040263 | 0.050828 | 00:04 | . 0.225451 .",
            "url": "https://doyu.github.io/blog/2022/08/18/fastbook09_tabular.html",
            "relUrl": "/2022/08/18/fastbook09_tabular.html",
            "date": " • Aug 18, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "Embeddeing == @One hot encoding",
            "content": "from fastai.collab import * from fastai.tabular.all import * . path = untar_data(URLs.ML_100k) path.ls() . (#23) [Path(&#39;/home/doyu/.fastai/data/ml-100k/ub.test&#39;),Path(&#39;/home/doyu/.fastai/data/ml-100k/u5.test&#39;),Path(&#39;/home/doyu/.fastai/data/ml-100k/u4.test&#39;),Path(&#39;/home/doyu/.fastai/data/ml-100k/u.occupation&#39;),Path(&#39;/home/doyu/.fastai/data/ml-100k/u.genre&#39;),Path(&#39;/home/doyu/.fastai/data/ml-100k/u.item&#39;),Path(&#39;/home/doyu/.fastai/data/ml-100k/ub.base&#39;),Path(&#39;/home/doyu/.fastai/data/ml-100k/u2.test&#39;),Path(&#39;/home/doyu/.fastai/data/ml-100k/README&#39;),Path(&#39;/home/doyu/.fastai/data/ml-100k/ua.test&#39;)...] . ratings = pd.read_csv(path/&#39;u.data&#39;, sep=&#39; t&#39;, header=None) ratings.columns = [&#39;user&#39;, &#39;movie&#39;, &#39;rating&#39;, &#39;timestamp&#39;] ratings = ratings.drop(columns=&#39;timestamp&#39;) ratings.head() . user movie rating . 0 196 | 242 | 3 | . 1 186 | 302 | 3 | . 2 22 | 377 | 1 | . 3 244 | 51 | 2 | . 4 166 | 346 | 1 | . dls = CollabDataLoaders.from_df(ratings, item_name=&#39;movie&#39;, bs=64) dls.show_batch() . user movie rating . 0 821 | 181 | 4 | . 1 495 | 140 | 5 | . 2 922 | 699 | 3 | . 3 548 | 472 | 2 | . 4 276 | 262 | 4 | . 5 432 | 3 | 3 | . 6 311 | 1222 | 3 | . 7 618 | 125 | 3 | . 8 715 | 735 | 4 | . 9 45 | 597 | 3 | . n_users = len(dls.classes[&#39;user&#39;]) n_movie = len(dls.classes[&#39;movie&#39;]) . n_factors = 5 user_factors = torch.randn(n_users, n_factors) movie_factors = torch.randn(n_movie, n_factors) user_factors.shape, movie_factors.shape . (torch.Size([944, 5]), torch.Size([1653, 5])) . one_hot_3 = one_hot(3, n_users).float() one_hot_5 = one_hot(5, n_users).float() one_hot = torch.stack([one_hot_3, one_hot_5], dim=1) one_hot.shape . torch.Size([944, 2]) . one_hot.t() @ user_factors . tensor([[ 0.1457, 0.3334, -1.6283, -1.3542, -1.1832], [-0.8224, 0.4091, 1.4074, 0.2336, -0.8085]]) . user_factors[[3,5],:] . tensor([[ 0.1457, 0.3334, -1.6283, -1.3542, -1.1832], [-0.8224, 0.4091, 1.4074, 0.2336, -0.8085]]) .",
            "url": "https://doyu.github.io/blog/2022/08/16/Embedding_vs_One_hot_encoding.html",
            "relUrl": "/2022/08/16/Embedding_vs_One_hot_encoding.html",
            "date": " • Aug 16, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "fastbook 08 How to create CrossTab from DataFrame",
            "content": "from fastai.collab import * from fastai.tabular.all import * path = untar_data(URLs.ML_100k) df = pd.read_csv(path/&#39;u.data&#39;, delimiter=&#39; t&#39;, header=None, names=[&#39;user&#39;, &#39;movie&#39;, &#39;rating&#39;, &#39;timestamp&#39;]) df = df.drop(columns=&#39;timestamp&#39;) df.head() . user movie rating . 0 196 | 242 | 3 | . 1 186 | 302 | 3 | . 2 22 | 377 | 1 | . 3 244 | 51 | 2 | . 4 166 | 346 | 1 | . Generating a new df only with most frequently reviewing users . user_ids = df.user.value_counts().index.tolist()[:10] df_users = df[df.user.isin(user_ids)] df_users.head() . user movie rating . 15 303 | 785 | 3 | . 19 234 | 1184 | 2 | . 38 276 | 796 | 1 | . 43 276 | 564 | 3 | . 63 13 | 526 | 3 | . Generating a new df only with most frequently reviewed movies . movie_ids = df.movie.value_counts().index.tolist()[:20] df_movies = df[df.movie.isin(movie_ids)] df_movies.head() . user movie rating . 12 200 | 222 | 5 | . 24 308 | 1 | 4 | . 31 301 | 98 | 4 | . 50 251 | 100 | 4 | . 53 25 | 181 | 5 | . Combine both dfs to generate an crosstable aligning on the above &#39;(index, user)&#39; and &#39;(index, movie)&#39; . pd.crosstab(df_users.user, df_movies.movie, values=df.rating, aggfunc=&#39;mean&#39;).fillna(&#39;-&#39;) . movie 1 7 50 56 98 100 117 121 127 172 174 181 204 222 237 258 286 288 294 300 . user . 13 3.0 | 2.0 | 5.0 | 5.0 | 4.0 | 5.0 | 3.0 | 5.0 | 5.0 | 5.0 | 4.0 | 5.0 | 5.0 | 3.0 | 5.0 | 4.0 | 3.0 | 1.0 | 2.0 | 1.0 | . 234 3.0 | 2.0 | 4.0 | 3.0 | 4.0 | 4.0 | 2.0 | - | 4.0 | 3.0 | 3.0 | 3.0 | 2.0 | 3.0 | 3.0 | 2.0 | 3.0 | 3.0 | 3.0 | 3.0 | . 276 5.0 | 5.0 | 5.0 | 5.0 | 5.0 | 5.0 | 4.0 | 4.0 | 5.0 | 5.0 | 5.0 | 5.0 | 5.0 | 4.0 | 5.0 | 5.0 | - | 4.0 | 4.0 | 4.0 | . 303 5.0 | 4.0 | 5.0 | 5.0 | 5.0 | 5.0 | 3.0 | 3.0 | 5.0 | 5.0 | 5.0 | 5.0 | 4.0 | 3.0 | 5.0 | 4.0 | 5.0 | 4.0 | 4.0 | 1.0 | . 393 3.0 | 4.0 | 5.0 | 2.0 | - | 1.0 | 4.0 | 4.0 | - | 5.0 | - | 4.0 | 4.0 | 4.0 | 4.0 | 4.0 | - | 3.0 | 4.0 | - | . 405 - | - | 5.0 | 4.0 | 4.0 | - | - | - | 5.0 | 5.0 | 5.0 | 5.0 | 5.0 | - | - | - | - | 5.0 | - | - | . 416 5.0 | 4.0 | 5.0 | 5.0 | 5.0 | 5.0 | 5.0 | 5.0 | 5.0 | 5.0 | 5.0 | 5.0 | 5.0 | - | 3.0 | 5.0 | 5.0 | 5.0 | 4.0 | 4.0 | . 450 4.0 | 4.0 | 5.0 | 4.0 | 4.0 | 4.0 | 4.0 | 3.0 | 5.0 | 4.0 | 5.0 | 4.0 | 4.0 | 3.0 | 5.0 | 4.0 | 4.0 | 3.0 | 4.0 | 4.0 | . 537 2.0 | 4.0 | 4.0 | 5.0 | 3.0 | 4.0 | 2.0 | 1.0 | 5.0 | 3.0 | 3.0 | 2.0 | 3.0 | 2.0 | 3.0 | 4.0 | 3.0 | 2.0 | 1.0 | 1.0 | . 655 2.0 | 3.0 | 4.0 | 3.0 | 4.0 | 3.0 | 2.0 | 3.0 | 5.0 | 4.0 | 3.0 | 3.0 | 3.0 | 2.0 | 3.0 | 2.0 | 3.0 | 3.0 | 3.0 | 3.0 | .",
            "url": "https://doyu.github.io/blog/2022/08/15/fastbook08-crosstab.html",
            "relUrl": "/2022/08/15/fastbook08-crosstab.html",
            "date": " • Aug 15, 2022"
        }
        
    
  
    
        ,"post8": {
            "title": "MovieLens with Collaboration Filtering from fastbook 08",
            "content": "from fastai.collab import * from fastai.tabular.all import * . path = untar_data(URLs.ML_100k) . ratings = pd.read_csv( path/&#39;u.data&#39;, delimiter=&#39; t&#39;, header=None, names=[&#39;user&#39;, &#39;movie&#39;, &#39;rating&#39;, &#39;timestamp&#39;]) ratings = ratings.drop(columns=&#39;timestamp&#39;) ratings.head() . user movie rating . 0 196 | 242 | 3 | . 1 186 | 302 | 3 | . 2 22 | 377 | 1 | . 3 244 | 51 | 2 | . 4 166 | 346 | 1 | . dls = CollabDataLoaders.from_df(ratings, item_name=&#39;movie&#39;, bs=64) dls.show_batch() . user movie rating . 0 622 | 190 | 4 | . 1 647 | 134 | 4 | . 2 786 | 174 | 4 | . 3 233 | 286 | 3 | . 4 405 | 1540 | 2 | . 5 301 | 404 | 3 | . 6 495 | 665 | 1 | . 7 222 | 1011 | 4 | . 8 940 | 746 | 3 | . 9 855 | 1021 | 3 | . L(dls.classes) . (#2) [&#39;user&#39;,&#39;movie&#39;] . n_users = len(dls.classes[&#39;user&#39;]) n_movies = len(dls.classes[&#39;movie&#39;]) n_factors = 5 user_factors = torch.randn(n_users, n_factors) movie_factors = torch.randn(n_movies, n_factors) . MSE for Dot products . class DotProduct(Module): def __init__(self, n_users, n_movies, n_factors): self.user_factors = Embedding(n_users, n_factors) self.movie_factors = Embedding(n_movies, n_factors) def forward(self, X): users = self.user_factors(X[:,0]) movies = self.movie_factors(X[:,1]) return (users * movies).sum(dim=1) . model = DotProduct(n_users, n_movies, 50) learn = Learner(dls, model, loss_func=MSELossFlat()) learn.fit_one_cycle(5, 5e-3) . epoch train_loss valid_loss time . 0 | 1.319163 | 1.301567 | 00:06 | . 1 | 1.072238 | 1.126393 | 00:06 | . 2 | 0.973071 | 1.006744 | 00:06 | . 3 | 0.819768 | 0.915777 | 00:06 | . 4 | 0.800278 | 0.895146 | 00:06 | . Specify Rating range . class DotProductRange(DotProduct): def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)): super().__init__(n_users, n_movies, n_factors) self.y_range = y_range def forward(self, X): return sigmoid_range(super().forward(X), *self.y_range) . model = DotProductRange(n_users, n_movies, 50) learn = Learner(dls, model, loss_func=MSELossFlat()) learn.fit_one_cycle(5, 5e-3) . epoch train_loss valid_loss time . 0 | 1.003102 | 0.991639 | 00:06 | . 1 | 0.872051 | 0.895169 | 00:06 | . 2 | 0.663116 | 0.866403 | 00:06 | . 3 | 0.477195 | 0.875070 | 00:06 | . 4 | 0.354954 | 0.879459 | 00:06 | . Add Bias respectively . class DotProductBias(DotProductRange): def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)): super().__init__(n_users, n_movies, n_factors, y_range=(0,5.5)) self.user_bias = Embedding(n_users, 1) self.movie_bias = Embedding(n_movies, 1) def foward(self, x): y = super().forward(X) y += self.user_bias(X[:,0]) y += self.movie_bias(X[:,1]) return sigmoid_range(y, *self.y_range) . model = DotProductBias(n_users, n_movies, 50) learn = Learner(dls, model, loss_func=MSELossFlat()) learn.fit_one_cycle(5, 5e-3) . epoch train_loss valid_loss time . 0 | 0.975578 | 0.985670 | 00:06 | . 1 | 0.892065 | 0.903127 | 00:06 | . 2 | 0.683729 | 0.865251 | 00:06 | . 3 | 0.478993 | 0.868494 | 00:06 | . 4 | 0.370724 | 0.872040 | 00:06 | . With Decay (L2 regularization) . model = DotProductBias(n_users, n_movies, 50) learn = Learner(dls, model, loss_func=MSELossFlat()) learn.fit_one_cycle(5, 5e-3, wd=0.1) . epoch train_loss valid_loss time . 0 | 0.975578 | 0.985670 | 00:06 | . 1 | 0.892065 | 0.903127 | 00:06 | . 2 | 0.683729 | 0.865251 | 00:06 | . 3 | 0.478993 | 0.868494 | 00:06 | . 4 | 0.370724 | 0.872040 | 00:06 | . Using fastai.collab . learn = collab_learner(dls, n_factors=50, y_range=(0, 5.5)) learn.fit_one_cycle(5, 5e-3, wd=0.1) . epoch train_loss valid_loss time . 0 | 0.963955 | 0.955790 | 00:07 | . 1 | 0.861782 | 0.877639 | 00:07 | . 2 | 0.738184 | 0.837129 | 00:07 | . 3 | 0.586984 | 0.824137 | 00:07 | . 4 | 0.497118 | 0.824534 | 00:07 | . learn.model . EmbeddingDotBias( (u_weight): Embedding(944, 50) (i_weight): Embedding(1655, 50) (u_bias): Embedding(944, 1) (i_bias): Embedding(1655, 1) ) . Deep Learning for Collaborative Filtering . embs = get_emb_sz(dls) embs . [(944, 74), (1655, 102)] . class CollabNN(Module): def __init__(self, user_sz, item_sz, y_range=(0, 5.5), n_act=100): self.user_factors = Embedding(*user_sz) self.item_factors = Embedding(*item_sz) self.layers = nn.Sequential( nn.Linear(user_sz[1]+item_sz[1], n_act), nn.ReLU(), nn.Linear(n_act, 1), ) self.y_range = y_range def forward(self, X): embs = self.user_factors(X[:,0]), self.item_factors(X[:,1]) x = self.layers(torch.cat(embs, dim=1)) return sigmoid_range(x, *self.y_range) . model = CollabNN(*embs) learn = Learner(dls, model, loss_func=MSELossFlat()) learn.fit_one_cycle(5, 5e-3, wd=0.01) . epoch train_loss valid_loss time . 0 | 0.967367 | 0.956959 | 00:07 | . 1 | 0.912710 | 0.909662 | 00:07 | . 2 | 0.835822 | 0.888609 | 00:06 | . 3 | 0.782297 | 0.876654 | 00:07 | . 4 | 0.781319 | 0.875355 | 00:07 | . learn = collab_learner(dls, use_nn=True, y_range=(0, 5.5), layers=[100]) learn.fit_one_cycle(5, 5e-3, wd=0.01) . epoch train_loss valid_loss time . 0 | 0.981277 | 0.965048 | 00:07 | . 1 | 0.905343 | 0.908017 | 00:07 | . 2 | 0.843490 | 0.867771 | 00:08 | . 3 | 0.821534 | 0.851313 | 00:07 | . 4 | 0.756023 | 0.853258 | 00:07 | .",
            "url": "https://doyu.github.io/blog/2022/08/15/collaborative_filtering.html",
            "relUrl": "/2022/08/15/collaborative_filtering.html",
            "date": " • Aug 15, 2022"
        }
        
    
  
    
        ,"post9": {
            "title": "fine_tune() vs fit_one_cycle()",
            "content": "from fastai.vision.all import * . path = untar_data(URLs.MNIST) path.ls() . . 100.03% [15687680/15683414 00:01&lt;00:00] (#2) [Path(&#39;/root/.fastai/data/mnist_png/training&#39;),Path(&#39;/root/.fastai/data/mnist_png/testing&#39;)] . Path.BASE_PATH = path path.ls(), (path/&#39;testing&#39;).ls() . ((#2) [Path(&#39;training&#39;),Path(&#39;testing&#39;)], (#10) [Path(&#39;testing/8&#39;),Path(&#39;testing/3&#39;),Path(&#39;testing/9&#39;),Path(&#39;testing/2&#39;),Path(&#39;testing/4&#39;),Path(&#39;testing/1&#39;),Path(&#39;testing/0&#39;),Path(&#39;testing/5&#39;),Path(&#39;testing/7&#39;),Path(&#39;testing/6&#39;)]) . fname = get_image_files(path) fname[:5], parent_label(fname[0]) . ((#5) [Path(&#39;training/8/23232.png&#39;),Path(&#39;training/8/3581.png&#39;),Path(&#39;training/8/27348.png&#39;),Path(&#39;training/8/35145.png&#39;),Path(&#39;training/8/13065.png&#39;)], &#39;8&#39;) . mnist = DataBlock( blocks = [ImageBlock(cls=PILImageBW), CategoryBlock], get_items=get_image_files, splitter=GrandparentSplitter(&#39;training&#39;, &#39;testing&#39;), get_y=parent_label) mnist.summary(path) . Setting-up type transforms pipelines Collecting items from /root/.fastai/data/mnist_png Found 70000 items 2 datasets of sizes 60000,10000 Setting up Pipeline: PILBase.create Setting up Pipeline: parent_label -&gt; Categorize -- {&#39;vocab&#39;: None, &#39;sort&#39;: True, &#39;add_na&#39;: False} Building one sample Pipeline: PILBase.create starting from /root/.fastai/data/mnist_png/training/8/23232.png applying PILBase.create gives PILImageBW mode=L size=28x28 Pipeline: parent_label -&gt; Categorize -- {&#39;vocab&#39;: None, &#39;sort&#39;: True, &#39;add_na&#39;: False} starting from /root/.fastai/data/mnist_png/training/8/23232.png applying parent_label gives 8 applying Categorize -- {&#39;vocab&#39;: None, &#39;sort&#39;: True, &#39;add_na&#39;: False} gives TensorCategory(8) Final sample: (PILImageBW mode=L size=28x28, TensorCategory(8)) Collecting items from /root/.fastai/data/mnist_png Found 70000 items 2 datasets of sizes 60000,10000 Setting up Pipeline: PILBase.create Setting up Pipeline: parent_label -&gt; Categorize -- {&#39;vocab&#39;: None, &#39;sort&#39;: True, &#39;add_na&#39;: False} Setting up after_item: Pipeline: ToTensor Setting up before_batch: Pipeline: Setting up after_batch: Pipeline: IntToFloatTensor -- {&#39;div&#39;: 255.0, &#39;div_mask&#39;: 1} Building one batch Applying item_tfms to the first sample: Pipeline: ToTensor starting from (PILImageBW mode=L size=28x28, TensorCategory(8)) applying ToTensor gives (TensorImageBW of size 1x28x28, TensorCategory(8)) Adding the next 3 samples No before_batch transform to apply Collating items in a batch Applying batch_tfms to the batch built Pipeline: IntToFloatTensor -- {&#39;div&#39;: 255.0, &#39;div_mask&#39;: 1} starting from (TensorImageBW of size 4x1x28x28, TensorCategory([8, 8, 8, 8], device=&#39;cuda:0&#39;)) applying IntToFloatTensor -- {&#39;div&#39;: 255.0, &#39;div_mask&#39;: 1} gives (TensorImageBW of size 4x1x28x28, TensorCategory([8, 8, 8, 8], device=&#39;cuda:0&#39;)) . dls = mnist.dataloaders(path, bs=256) . dls.show_batch(ncols=9, figsize=(4,4)) . X, y = dls.valid.one_batch() X.shape, y.shape, X[0,0,:14,14], y[:9] . (torch.Size([256, 1, 28, 28]), torch.Size([256]), TensorImageBW([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3922, 0.9882, 0.9647, 0.2392, 0.0000, 0.0157, 0.6118, 0.9882], device=&#39;cuda:0&#39;), TensorCategory([8, 8, 8, 8, 8, 8, 8, 8, 8], device=&#39;cuda:0&#39;)) . Vanilla fine_tune() . learn = vision_learner(dls, resnet18, metrics=accuracy) learn.fine_tune(1, base_lr=0.1) . epoch train_loss valid_loss accuracy time . 0 | 0.268339 | 0.522245 | 0.912200 | 00:29 | . epoch train_loss valid_loss accuracy time . 0 | 0.062585 | 0.033629 | 0.990700 | 00:29 | . fine_tune() with lr_find() . learn = vision_learner(dls, resnet18, metrics=accuracy) #lr_min,lr_steep = learn.lr_find(suggest_funcs=(minimum, steep)) learn.lr_find() . SuggestedLRs(valley=0.0010000000474974513) . learn.fine_tune(1, base_lr=0.03) . epoch train_loss valid_loss accuracy time . 0 | 0.077246 | 0.094525 | 0.982400 | 00:27 | . epoch train_loss valid_loss accuracy time . 0 | 0.055067 | 0.030450 | 0.991000 | 00:28 | . fit_one_cycle() after lr_find() . learn = vision_learner(dls, resnet18, metrics=accuracy) learn.fit_one_cycle(3, 0.03) learn.unfreeze() learn.lr_find() . epoch train_loss valid_loss accuracy time . 0 | 0.165410 | 0.146592 | 0.962900 | 00:25 | . 1 | 0.071531 | 0.056035 | 0.984200 | 00:22 | . 2 | 0.039154 | 0.029686 | 0.990500 | 00:26 | . SuggestedLRs(valley=3.981071586167673e-06) . learn.fit_one_cycle(6, lr_max=5e-5) . epoch train_loss valid_loss accuracy time . 0 | 0.028148 | 0.027282 | 0.990700 | 00:25 | . 1 | 0.028248 | 0.026331 | 0.991300 | 00:26 | . 2 | 0.017925 | 0.026180 | 0.992400 | 00:27 | . 3 | 0.012465 | 0.024204 | 0.992600 | 00:25 | . 4 | 0.006333 | 0.023429 | 0.993100 | 00:26 | . 5 | 0.005000 | 0.023680 | 0.993600 | 00:28 | . Discriminative LR . learn = vision_learner(dls, resnet18, metrics=accuracy) learn.fit_one_cycle(3, 0.03) learn.unfreeze() learn.fit_one_cycle(12, lr_max=slice(1e-6,1e-4)) learn.recorder.plot_loss() . epoch train_loss valid_loss accuracy time . 0 | 0.178551 | 0.122690 | 0.964300 | 00:24 | . 1 | 0.076595 | 0.047100 | 0.986100 | 00:24 | . 2 | 0.037789 | 0.027752 | 0.992100 | 00:26 | . epoch train_loss valid_loss accuracy time . 0 | 0.026517 | 0.027263 | 0.992300 | 00:24 | . 1 | 0.027518 | 0.026565 | 0.992000 | 00:23 | . 2 | 0.024748 | 0.025845 | 0.992500 | 00:26 | . 3 | 0.023407 | 0.025002 | 0.992700 | 00:23 | . 4 | 0.015296 | 0.024018 | 0.992600 | 00:28 | . 5 | 0.015998 | 0.024011 | 0.992700 | 00:30 | . 6 | 0.012829 | 0.024593 | 0.992800 | 00:29 | . 7 | 0.012620 | 0.025312 | 0.992600 | 00:31 | . 8 | 0.012434 | 0.024796 | 0.992800 | 00:33 | . 9 | 0.009450 | 0.025069 | 0.992800 | 00:32 | . 10 | 0.009843 | 0.024491 | 0.992900 | 00:29 | . 11 | 0.009071 | 0.024356 | 0.993100 | 00:27 | . to_fp16() + fine_tune(free_epochs) . from fastai.callback.fp16 import * learn = vision_learner(dls, resnet50, metrics=accuracy).to_fp16() learn.fine_tune(6, freeze_epochs=3) . epoch train_loss valid_loss accuracy time . 0 | 1.094105 | 0.660184 | 0.792900 | 00:23 | . 1 | 0.612468 | 0.398040 | 0.876300 | 00:33 | . 2 | 0.340697 | 0.232054 | 0.925600 | 00:28 | . epoch train_loss valid_loss accuracy time . 0 | 0.117487 | 0.071663 | 0.977600 | 00:31 | . 1 | 0.061673 | 0.049561 | 0.984400 | 00:28 | . 2 | 0.031958 | 0.038238 | 0.988400 | 00:29 | . 3 | 0.016311 | 0.034266 | 0.990400 | 00:29 | . 4 | 0.003950 | 0.028339 | 0.992500 | 00:32 | . 5 | 0.000928 | 0.026444 | 0.993000 | 00:29 | .",
            "url": "https://doyu.github.io/blog/2022/08/09/improving-model-fastbook05.html",
            "relUrl": "/2022/08/09/improving-model-fastbook05.html",
            "date": " • Aug 9, 2022"
        }
        
    
  
    
        ,"post10": {
            "title": "nn.BCELoss()",
            "content": "Preparation . from fastai.vision.all import * . batch_size = 5 n_classes = 3 . t = [ [0, 1, 1], [1, 1, 1], [0, 0, 0], [1, 0, 0], [0, 1, 0], ] #torch.stack([Tensor(y) for y in x] for x in t) t = torch.tensor(t, dtype=torch.long) t . tensor([[0, 1, 1], [1, 1, 1], [0, 0, 0], [1, 0, 0], [0, 1, 0]]) . W = torch.randn(9, n_classes) b = torch.randn(1) X = torch.randn(batch_size, 9) y = F.relu(X@W + b) y.shape, y . (torch.Size([5, 3]), tensor([[4.0340, 0.0000, 3.0853], [3.1036, 0.0000, 0.0000], [0.0000, 1.1581, 0.0000], [0.0000, 0.0000, 0.0000], [1.3523, 0.0000, 1.1482]])) . nn.CrossEntropyLoss() . y.shape, t.shape . (torch.Size([5, 3]), torch.Size([5, 3])) . loss = nn.BCEWithLogitsLoss()(y, t.float()) loss . tensor(0.9877) . y0 = y.sigmoid() y0 . tensor([[0.9826, 0.5000, 0.9563], [0.9570, 0.5000, 0.5000], [0.5000, 0.7610, 0.5000], [0.5000, 0.5000, 0.5000], [0.7945, 0.5000, 0.7592]]) . t . tensor([[0, 1, 1], [1, 1, 1], [0, 0, 0], [1, 0, 0], [0, 1, 0]]) . -torch.where(t==1, y0, 1-y0).log().mean() . tensor(0.9877) . Accuracy for BCE . def accum(y, tgt, thr): return ((y.sigmoid()&gt;thr)==tgt).float().mean() . xs = torch.linspace(0.05,0.95,29) accs = [accum(y, t, thr=i) for i in xs] plt.plot(xs,accs) . [&lt;matplotlib.lines.Line2D at 0x7fcbd50f42e0&gt;] .",
            "url": "https://doyu.github.io/blog/2022/08/09/binary-cross_entropy_loss.html",
            "relUrl": "/2022/08/09/binary-cross_entropy_loss.html",
            "date": " • Aug 9, 2022"
        }
        
    
  
    
        ,"post11": {
            "title": "nn.CrossEntropyLoss, F.softmax, F.log_softmax, F.nll_loss()",
            "content": "from fastai.vision.all import * . batch_size = 5 n_classes = 3 W = torch.randn(9, n_classes) b = torch.randn(1) X = torch.randn(batch_size, 9) y = F.relu(X@W + b) y.shape, y . (torch.Size([5, 3]), tensor([[1.6193, 0.2690, 0.0000], [0.0000, 2.2364, 0.0000], [6.1974, 0.0000, 3.0223], [0.0000, 0.0000, 3.4538], [2.7525, 0.0000, 2.7033]])) . target {0,1,2} . t = torch.empty(batch_size, dtype=torch.long).random_(n_classes) t . tensor([2, 2, 2, 1, 2]) . loss = nn.CrossEntropyLoss()(y, t) loss . tensor(2.3818) . F.nll_loss(F.log_softmax(y, dim=1), t) . tensor(2.3818) . y.softmax(dim=1) . tensor([[0.6863, 0.1778, 0.1359], [0.0880, 0.8239, 0.0880], [0.9580, 0.0019, 0.0400], [0.0297, 0.0297, 0.9405], [0.4961, 0.0316, 0.4723]]) . y.softmax(dim=1).sum(dim=1) . tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000]) . y.log_softmax(dim=1).sum(dim=1) . tensor([-4.0992, -5.0538, -9.5012, -7.0915, -4.9047]) . y.softmax(dim=1).log().sum(dim=1) . tensor([-4.0992, -5.0538, -9.5012, -7.0915, -4.9047]) . F.nll_loss(y.softmax(dim=1).log(), t) . tensor(2.3818) . y0 = y.softmax(dim=1) y0 = y0.log() y0 = y0[range(batch_size), t.tolist()] y0 = -y0.mean() y0 . tensor(2.3818) . nn.CrossEntropyLoss()(y, t) . tensor(2.3818) .",
            "url": "https://doyu.github.io/blog/2022/08/08/cross_entropy_loss.html",
            "relUrl": "/2022/08/08/cross_entropy_loss.html",
            "date": " • Aug 8, 2022"
        }
        
    
  
    
        ,"post12": {
            "title": "Comprehensive Image Classification Introduction @fastbook 05",
            "content": "Image Classification . From Dogs and Cats to Pet Breeds . from fastai.vision.all import * path = untar_data(URLs.PETS) Path.BASE_PATH = path path.ls(), (path/&quot;images&quot;).ls() . ((#2) [Path(&#39;images&#39;),Path(&#39;annotations&#39;)], (#7393) [Path(&#39;images/miniature_pinscher_199.jpg&#39;),Path(&#39;images/newfoundland_183.jpg&#39;),Path(&#39;images/pomeranian_90.jpg&#39;),Path(&#39;images/pomeranian_102.jpg&#39;),Path(&#39;images/japanese_chin_74.jpg&#39;),Path(&#39;images/yorkshire_terrier_45.jpg&#39;),Path(&#39;images/chihuahua_34.jpg&#39;),Path(&#39;images/american_pit_bull_terrier_150.jpg&#39;),Path(&#39;images/wheaten_terrier_160.jpg&#39;),Path(&#39;images/staffordshire_bull_terrier_91.jpg&#39;)...]) . fname = (path/&quot;images&quot;).ls()[0] fname, re.findall(r&#39;(.+)_ d+.jpg$&#39;, fname.name) . (Path(&#39;images/miniature_pinscher_199.jpg&#39;), [&#39;miniature_pinscher&#39;]) . pets = DataBlock( blocks = (ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(seed=42), get_y=using_attr(RegexLabeller(r&#39;(.+)_ d+.jpg$&#39;), &#39;name&#39;), item_tfms=Resize(460), batch_tfms=aug_transforms(size=224, min_scale=0.75) ) dls = pets.dataloaders(path/&quot;images&quot;) . f = using_attr(RegexLabeller(r&#39;(.+)_ d+.jpg$&#39;), &#39;name&#39;) f(fname) . &#39;miniature_pinscher&#39; . Presizing . %cd /notebooks/fastbook . /notebooks/fastbook . dblock1 = DataBlock( blocks=( ImageBlock(), CategoryBlock() ), get_y=parent_label, item_tfms=Resize(460) ) # Place an image in the &#39;images/grizzly.jpg&#39; subfolder where this notebook is located before running this dls1 = dblock1.dataloaders([(Path.cwd()/&#39;images&#39;/&#39;grizzly.jpg&#39;)]*100, bs=8) dls1.train.get_idxs = lambda: Inf.ones x,y = dls1.valid.one_batch() _,axs = subplots(1, 2) x1 = TensorImage(x.clone()) x1 = x1.affine_coord(sz=224) x1 = x1.rotate(draw=30, p=1.) x1 = x1.zoom(draw=1.2, p=1.) x1 = x1.warp(draw_x=-0.2, draw_y=0.2, p=1.) tfms = setup_aug_tfms([ Rotate(draw=30, p=1, size=224), Zoom(draw=1.2, p=1., size=224), Warp(draw_x=-0.2, draw_y=0.2, p=1., size=224) ]) x = Pipeline(tfms)(x) #x.affine_coord(coord_tfm=coord_tfm, sz=size, mode=mode, pad_mode=pad_mode) TensorImage(x[0]).show(ctx=axs[0]) TensorImage(x1[0]).show(ctx=axs[1]); . How &#39;parent_label()&#39; works . fname, parent_label(fname) . (Path(&#39;images/miniature_pinscher_199.jpg&#39;), &#39;images&#39;) . Path.cwd(), Path.cwd().ls() . (Path(&#39;/notebooks/fastbook&#39;), (#66) [Path(&#39;/notebooks/fastbook/requirements.txt&#39;),Path(&#39;/notebooks/fastbook/08_collab.py&#39;),Path(&#39;/notebooks/fastbook/CODE_OF_CONDUCT.md&#39;),Path(&#39;/notebooks/fastbook/03_ethics.py&#39;),Path(&#39;/notebooks/fastbook/README_es.md&#39;),Path(&#39;/notebooks/fastbook/11_midlevel_data.py&#39;),Path(&#39;/notebooks/fastbook/20_conclusion.ipynb&#39;),Path(&#39;/notebooks/fastbook/14_resnet.py&#39;),Path(&#39;/notebooks/fastbook/README_vn.md&#39;),Path(&#39;/notebooks/fastbook/16_accel_sgd.py&#39;)...]) . Checking and Debugging a DataBlock . dls.show_batch(nrows=1, ncols=3) . added &#39;Resize(460)&#39; for GPU, Otherwise &#39;summary()&#39; doesn&#39;t work . pets1 = DataBlock(blocks = (ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(seed=42), get_y=using_attr(RegexLabeller(r&#39;(.+)_ d+.jpg$&#39;), &#39;name&#39;), item_tfms=Resize(460)) # same size for GPU pets1.summary(path/&quot;images&quot;) . Setting-up type transforms pipelines Collecting items from /root/.fastai/data/oxford-iiit-pet/images Found 7390 items 2 datasets of sizes 5912,1478 Setting up Pipeline: PILBase.create Setting up Pipeline: partial -&gt; Categorize -- {&#39;vocab&#39;: None, &#39;sort&#39;: True, &#39;add_na&#39;: False} Building one sample Pipeline: PILBase.create starting from /root/.fastai/data/oxford-iiit-pet/images/shiba_inu_180.jpg applying PILBase.create gives PILImage mode=RGB size=500x375 Pipeline: partial -&gt; Categorize -- {&#39;vocab&#39;: None, &#39;sort&#39;: True, &#39;add_na&#39;: False} starting from /root/.fastai/data/oxford-iiit-pet/images/shiba_inu_180.jpg applying partial gives shiba_inu applying Categorize -- {&#39;vocab&#39;: None, &#39;sort&#39;: True, &#39;add_na&#39;: False} gives TensorCategory(33) Final sample: (PILImage mode=RGB size=500x375, TensorCategory(33)) Collecting items from /root/.fastai/data/oxford-iiit-pet/images Found 7390 items 2 datasets of sizes 5912,1478 Setting up Pipeline: PILBase.create Setting up Pipeline: partial -&gt; Categorize -- {&#39;vocab&#39;: None, &#39;sort&#39;: True, &#39;add_na&#39;: False} Setting up after_item: Pipeline: Resize -- {&#39;size&#39;: (460, 460), &#39;method&#39;: &#39;crop&#39;, &#39;pad_mode&#39;: &#39;reflection&#39;, &#39;resamples&#39;: (&lt;Resampling.BILINEAR: 2&gt;, &lt;Resampling.NEAREST: 0&gt;), &#39;p&#39;: 1.0} -&gt; ToTensor Setting up before_batch: Pipeline: Setting up after_batch: Pipeline: IntToFloatTensor -- {&#39;div&#39;: 255.0, &#39;div_mask&#39;: 1} Building one batch Applying item_tfms to the first sample: Pipeline: Resize -- {&#39;size&#39;: (460, 460), &#39;method&#39;: &#39;crop&#39;, &#39;pad_mode&#39;: &#39;reflection&#39;, &#39;resamples&#39;: (&lt;Resampling.BILINEAR: 2&gt;, &lt;Resampling.NEAREST: 0&gt;), &#39;p&#39;: 1.0} -&gt; ToTensor starting from (PILImage mode=RGB size=500x375, TensorCategory(33)) applying Resize -- {&#39;size&#39;: (460, 460), &#39;method&#39;: &#39;crop&#39;, &#39;pad_mode&#39;: &#39;reflection&#39;, &#39;resamples&#39;: (&lt;Resampling.BILINEAR: 2&gt;, &lt;Resampling.NEAREST: 0&gt;), &#39;p&#39;: 1.0} gives (PILImage mode=RGB size=460x460, TensorCategory(33)) applying ToTensor gives (TensorImage of size 3x460x460, TensorCategory(33)) Adding the next 3 samples No before_batch transform to apply Collating items in a batch Applying batch_tfms to the batch built Pipeline: IntToFloatTensor -- {&#39;div&#39;: 255.0, &#39;div_mask&#39;: 1} starting from (TensorImage of size 4x3x460x460, TensorCategory([33, 23, 18, 4], device=&#39;cuda:0&#39;)) applying IntToFloatTensor -- {&#39;div&#39;: 255.0, &#39;div_mask&#39;: 1} gives (TensorImage of size 4x3x460x460, TensorCategory([33, 23, 18, 4], device=&#39;cuda:0&#39;)) . learn = vision_learner(dls, resnet34, metrics=error_rate) learn.fine_tune(2) . epoch train_loss valid_loss error_rate time . 0 | 1.481196 | 0.374433 | 0.122463 | 00:29 | . epoch train_loss valid_loss error_rate time . 0 | 0.514027 | 0.305406 | 0.100135 | 00:38 | . 1 | 0.338844 | 0.226652 | 0.073748 | 00:39 | . Cross-Entropy Loss . Viewing Activations and Labels . dls.vocab[2] . &#39;Birman&#39; . x,y = dls.one_batch() preds,y1 = learn.get_preds(dl=[(x,y)]) y, preds[0] . (TensorCategory([14, 6, 24, 33, 2, 15, 7, 10, 19, 34, 16, 8, 10, 0, 34, 13, 31, 13, 26, 30, 2, 9, 17, 32, 26, 14, 14, 32, 24, 34, 28, 16, 6, 14, 3, 10, 34, 5, 5, 11, 36, 24, 35, 11, 30, 35, 23, 3, 34, 0, 16, 17, 23, 2, 23, 29, 22, 13, 27, 11, 25, 8, 18, 29], device=&#39;cuda:0&#39;), TensorBase([7.2221e-04, 5.1115e-05, 3.6684e-05, 3.7296e-06, 3.6732e-04, 1.9829e-04, 1.9351e-05, 7.4164e-05, 1.5373e-06, 3.3986e-04, 5.7419e-06, 8.2857e-06, 2.3575e-04, 2.1627e-04, 9.8651e-01, 2.7815e-03, 3.9936e-05, 2.9327e-06, 3.8810e-03, 7.3829e-05, 4.0817e-04, 4.3759e-04, 1.2597e-05, 7.3710e-06, 2.7713e-06, 1.2360e-04, 5.5445e-05, 1.8330e-04, 3.3123e-05, 2.6849e-05, 9.8329e-05, 3.2327e-05, 4.6130e-05, 6.5665e-05, 9.1696e-05, 2.7759e-03, 2.4891e-05])) . len(preds[0]),preds[0].sum() . (37, TensorBase(1.)) . Softmax . plot_function(torch.sigmoid, min=-4,max=4) . plot_function(torch.exp, min=-4,max=4) . acts = torch.randn((6,2))*2 acts . tensor([[ 0.6734, 0.2576], [ 0.4689, 0.4607], [-2.2457, -0.3727], [ 4.4164, -1.2760], [ 0.9233, 0.5347], [ 1.0698, 1.6187]]) . acts.sigmoid() . tensor([[0.6623, 0.5641], [0.6151, 0.6132], [0.0957, 0.4079], [0.9881, 0.2182], [0.7157, 0.6306], [0.7446, 0.8346]]) . torch.exp(acts[:,0]) / torch.exp(acts).sum(dim=1) . tensor([0.6025, 0.5021, 0.1332, 0.9966, 0.5959, 0.3661]) . sm_acts = torch.softmax(acts, dim=1) sm_acts . tensor([[0.6025, 0.3975], [0.5021, 0.4979], [0.1332, 0.8668], [0.9966, 0.0034], [0.5959, 0.4041], [0.3661, 0.6339]]) . sm_acts.sum(dim=1) . tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]) . Log Likelihood . targ = tensor([0,1,0,1,1,0]) . sm_acts . tensor([[0.6025, 0.3975], [0.5021, 0.4979], [0.1332, 0.8668], [0.9966, 0.0034], [0.5959, 0.4041], [0.3661, 0.6339]]) . idx = range(6) sm_acts[idx, targ] . tensor([0.6025, 0.4979, 0.1332, 0.0034, 0.4041, 0.3661]) . from IPython.display import HTML df = pd.DataFrame(sm_acts, columns=[&quot;3&quot;,&quot;7&quot;]) df[&#39;targ&#39;] = targ df[&#39;idx&#39;] = idx df[&#39;result&#39;] = sm_acts[range(6), targ] t = df.style.hide_index() #To have html code compatible with our script html = t._repr_html_().split(&#39;&lt;/style&gt;&#39;)[1] html = re.sub(r&#39;&lt;table id=&quot;([^&quot;]+)&quot; s*&gt;&#39;, r&#39;&lt;table &gt;&#39;, html) display(HTML(html)) . /tmp/ipykernel_5100/1219388098.py:6: FutureWarning: this method is deprecated in favour of `Styler.hide(axis=&#39;index&#39;)` t = df.style.hide_index() . 3 7 targ idx result . 0.602469 | 0.397531 | 0 | 0 | 0.602469 | . 0.502065 | 0.497935 | 1 | 1 | 0.497935 | . 0.133188 | 0.866811 | 0 | 2 | 0.133188 | . 0.996640 | 0.003360 | 1 | 3 | 0.003360 | . 0.595949 | 0.404051 | 1 | 4 | 0.404051 | . 0.366118 | 0.633882 | 0 | 5 | 0.366118 | . -sm_acts[idx, targ] . tensor([-0.6025, -0.4979, -0.1332, -0.0034, -0.4041, -0.3661]) . F.nll_loss(sm_acts, targ, reduction=&#39;none&#39;) . tensor([-0.6025, -0.4979, -0.1332, -0.0034, -0.4041, -0.3661]) . Taking the Log . Recall that cross entropy loss may involve the multiplication of many numbers. Multiplying lots of negative numbers together can cause problems like numerical underflow in computers. Therefore, we want to transform these probabilities to larger values so we can perform mathematical operations on them. There is a mathematical function that does exactly this: the logarithm (available as torch.log). It is not defined for numbers less than 0, and looks like this between 0 and 1: . plot_function(torch.log, min=0,max=1, ty=&#39;log(x)&#39;, tx=&#39;x&#39;) . plot_function(lambda x: -1*torch.log(x), min=0,max=1, tx=&#39;x&#39;, ty=&#39;- log(x)&#39;, title = &#39;Log Loss when true label = 1&#39;) . from IPython.display import HTML df[&#39;loss&#39;] = -torch.log(tensor(df[&#39;result&#39;])) t = df.style.hide_index() #To have html code compatible with our script html = t._repr_html_().split(&#39;&lt;/style&gt;&#39;)[1] html = re.sub(r&#39;&lt;table id=&quot;([^&quot;]+)&quot; s*&gt;&#39;, r&#39;&lt;table &gt;&#39;, html) display(HTML(html)) . /tmp/ipykernel_5100/2201212877.py:3: FutureWarning: this method is deprecated in favour of `Styler.hide(axis=&#39;index&#39;)` t = df.style.hide_index() . 3 7 targ idx result loss . 0.602469 | 0.397531 | 0 | 0 | 0.602469 | 0.506720 | . 0.502065 | 0.497935 | 1 | 1 | 0.497935 | 0.697285 | . 0.133188 | 0.866811 | 0 | 2 | 0.133188 | 2.015990 | . 0.996640 | 0.003360 | 1 | 3 | 0.003360 | 5.695763 | . 0.595949 | 0.404051 | 1 | 4 | 0.404051 | 0.906213 | . 0.366118 | 0.633882 | 0 | 5 | 0.366118 | 1.004798 | . Negative Log Likelihood . loss_func = nn.CrossEntropyLoss() . loss_func(acts, targ) . tensor(1.8045) . F.cross_entropy(acts, targ) . tensor(1.8045) . nn.CrossEntropyLoss(reduction=&#39;none&#39;)(acts, targ) . tensor([0.5067, 0.6973, 2.0160, 5.6958, 0.9062, 1.0048]) . acts, targ . (tensor([[ 0.6734, 0.2576], [ 0.4689, 0.4607], [-2.2457, -0.3727], [ 4.4164, -1.2760], [ 0.9233, 0.5347], [ 1.0698, 1.6187]]), tensor([0, 1, 0, 1, 1, 0])) . Model Interpretation . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix(figsize=(12,12), dpi=60) . interp.most_confused(min_val=5) . [(&#39;Bengal&#39;, &#39;Egyptian_Mau&#39;, 9), (&#39;american_pit_bull_terrier&#39;, &#39;staffordshire_bull_terrier&#39;, 7), (&#39;basset_hound&#39;, &#39;beagle&#39;, 5)] . Improving Our Model . The Learning Rate Finder . learn = vision_learner(dls, resnet34, metrics=error_rate) learn.fine_tune(1, base_lr=0.1) . epoch train_loss valid_loss error_rate time . 0 | 2.801034 | 7.120271 | 0.574425 | 00:30 | . epoch train_loss valid_loss error_rate time . 0 | 3.409698 | 1.739531 | 0.510825 | 00:38 | . learn = vision_learner(dls, resnet34, metrics=error_rate) lr_min,lr_steep = learn.lr_find(suggest_funcs=(minimum, steep)) . print(f&quot;Minimum/10: {lr_min:.2e}, steepest point: {lr_steep:.2e}&quot;) . Minimum/10: 8.32e-03, steepest point: 4.37e-03 . (lr_min + lr_steep)/2. . 0.006341397855430841 . learn = vision_learner(dls, resnet34, metrics=error_rate) learn.fine_tune(3, base_lr=3e-3) . epoch train_loss valid_loss error_rate time . 0 | 1.287065 | 0.331208 | 0.102165 | 00:29 | . epoch train_loss valid_loss error_rate time . 0 | 0.522278 | 0.414550 | 0.127876 | 00:38 | . 1 | 0.402920 | 0.259237 | 0.082544 | 00:38 | . 2 | 0.214323 | 0.236262 | 0.073748 | 00:39 | . learn.recorder.plot_loss() . plt.plot(L((learn.recorder).values).itemgot(2)) . [&lt;matplotlib.lines.Line2D at 0x7f1fd92f8fa0&gt;] . Unfreezing and Transfer Learning . learn.fine_tune?? . Signature: learn.fine_tune( epochs, base_lr=0.002, freeze_epochs=1, lr_mult=100, pct_start=0.3, div=5.0, lr_max=None, div_final=100000.0, wd=None, moms=None, cbs=None, reset_opt=False, start_epoch=0, ) Source: @patch @delegates(Learner.fit_one_cycle) def fine_tune(self:Learner, epochs, base_lr=2e-3, freeze_epochs=1, lr_mult=100, pct_start=0.3, div=5.0, **kwargs): &#34;Fine tune with `Learner.freeze` for `freeze_epochs`, then with `Learner.unfreeze` for `epochs`, using discriminative LR.&#34; self.freeze() self.fit_one_cycle(freeze_epochs, slice(base_lr), pct_start=0.99, **kwargs) base_lr /= 2 self.unfreeze() self.fit_one_cycle(epochs, slice(base_lr/lr_mult, base_lr), pct_start=pct_start, div=div, **kwargs) File: ~/mambaforge/lib/python3.9/site-packages/fastai/callback/schedule.py Type: method . learn = vision_learner(dls, resnet34, metrics=error_rate) learn.fit_one_cycle(3, 3e-3) . epoch train_loss valid_loss error_rate time . 0 | 1.136077 | 0.414258 | 0.139378 | 00:29 | . 1 | 0.509912 | 0.293178 | 0.089986 | 00:29 | . 2 | 0.330361 | 0.248266 | 0.076455 | 00:29 | . learn.unfreeze() . learn.lr_find() . SuggestedLRs(valley=6.918309736647643e-06) . learn.fit_one_cycle(6, lr_max=1e-5) . epoch train_loss valid_loss error_rate time . 0 | 0.263112 | 0.242040 | 0.077131 | 00:38 | . 1 | 0.232440 | 0.230828 | 0.069689 | 00:39 | . 2 | 0.217150 | 0.227805 | 0.069689 | 00:39 | . 3 | 0.206957 | 0.220832 | 0.071042 | 00:39 | . 4 | 0.202025 | 0.217777 | 0.070365 | 00:39 | . 5 | 0.178928 | 0.219291 | 0.066306 | 00:40 | . learn.recorder.plot_loss() . Discriminative Learning Rates . learn = vision_learner(dls, resnet34, metrics=error_rate) learn.fit_one_cycle(3, 3e-3) learn.unfreeze() learn.fit_one_cycle(12, lr_max=slice(1e-6,1e-4)) . epoch train_loss valid_loss error_rate time . 0 | 1.125234 | 0.340846 | 0.100135 | 00:30 | . 1 | 0.538148 | 0.255423 | 0.079161 | 00:30 | . 2 | 0.338863 | 0.227787 | 0.072395 | 00:29 | . epoch train_loss valid_loss error_rate time . 0 | 0.270723 | 0.225311 | 0.074425 | 00:39 | . 1 | 0.258127 | 0.216694 | 0.071042 | 00:39 | . 2 | 0.249060 | 0.215301 | 0.069689 | 00:39 | . 3 | 0.208266 | 0.211926 | 0.065629 | 00:39 | . 4 | 0.184873 | 0.213426 | 0.067659 | 00:39 | . 5 | 0.168494 | 0.208151 | 0.066306 | 00:39 | . 6 | 0.158416 | 0.201037 | 0.066306 | 00:39 | . 7 | 0.161552 | 0.205724 | 0.063599 | 00:39 | . 8 | 0.138080 | 0.204028 | 0.060217 | 00:39 | . 9 | 0.133944 | 0.204857 | 0.064276 | 00:39 | . 10 | 0.125342 | 0.200153 | 0.064276 | 00:39 | . 11 | 0.134450 | 0.201896 | 0.063599 | 00:39 | . plt.plot(L(learn.recorder.values).itemgot(2)) . [&lt;matplotlib.lines.Line2D at 0x7f1f430fd670&gt;] . learn.recorder.plot_loss() . Selecting the Number of Epochs . Deeper Architectures . from fastai.callback.fp16 import * learn = vision_learner(dls, resnet50, metrics=error_rate).to_fp16() learn.fine_tune(6, freeze_epochs=3) . Downloading: &#34;https://download.pytorch.org/models/resnet50-0676ba61.pth&#34; to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth . epoch train_loss valid_loss error_rate time . 0 | 1.309392 | 0.298439 | 0.094046 | 00:24 | . 1 | 0.599204 | 0.314211 | 0.095399 | 00:24 | . 2 | 0.431089 | 0.283283 | 0.087280 | 00:24 | . epoch train_loss valid_loss error_rate time . 0 | 0.285965 | 0.235522 | 0.073748 | 00:29 | . 1 | 0.303820 | 0.411849 | 0.108254 | 00:29 | . 2 | 0.256097 | 0.265159 | 0.077131 | 00:28 | . 3 | 0.158060 | 0.272260 | 0.075101 | 00:28 | . 4 | 0.100597 | 0.207765 | 0.060217 | 00:29 | . 5 | 0.060010 | 0.197659 | 0.061570 | 00:29 | . Conclusion . Questionnaire . Why do we first resize to a large size on the CPU, and then to a smaller size on the GPU? | If you are not familiar with regular expressions, find a regular expression tutorial, and some problem sets, and complete them. Have a look on the book&#39;s website for suggestions. | What are the two ways in which data is most commonly provided, for most deep learning datasets? | Look up the documentation for L and try using a few of the new methods that it adds. | Look up the documentation for the Python pathlib module and try using a few methods of the Path class. | Give two examples of ways that image transformations can degrade the quality of the data. | What method does fastai provide to view the data in a DataLoaders? | What method does fastai provide to help you debug a DataBlock? | Should you hold off on training a model until you have thoroughly cleaned your data? | What are the two pieces that are combined into cross-entropy loss in PyTorch? | What are the two properties of activations that softmax ensures? Why is this important? | When might you want your activations to not have these two properties? | Calculate the exp and softmax columns of &lt;&gt; yourself (i.e., in a spreadsheet, with a calculator, or in a notebook).&lt;/li&gt; Why can&#39;t we use torch.where to create a loss function for datasets where our label can have more than two categories? | What is the value of log(-2)? Why? | What are two good rules of thumb for picking a learning rate from the learning rate finder? | What two steps does the fine_tune method do? | In Jupyter Notebook, how do you get the source code for a method or function? | What are discriminative learning rates? | How is a Python slice object interpreted when passed as a learning rate to fastai? | Why is early stopping a poor choice when using 1cycle training? | What is the difference between resnet50 and resnet101? | What does to_fp16 do? | &lt;/ol&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Further Research . Find the paper by Leslie Smith that introduced the learning rate finder, and read it. | See if you can improve the accuracy of the classifier in this chapter. What&#39;s the best accuracy you can achieve? Look on the forums and the book&#39;s website to see what other students have achieved with this dataset, and how they did it. | &lt;/div&gt; |",
            "url": "https://doyu.github.io/blog/2022/08/08/DL_image_classification.html",
            "relUrl": "/2022/08/08/DL_image_classification.html",
            "date": " • Aug 8, 2022"
        }
        
    
  
    
        ,"post13": {
            "title": "DataLoader from DataBlock for MNIST_SAMPLE",
            "content": "from fastai.vision.all import * . path = untar_data(URLs.MNIST_SAMPLE) . mnist = DataBlock( blocks=(ImageBlock(PILImageBW), CategoryBlock), get_items=get_image_files, splitter=GrandparentSplitter(), get_y=parent_label ) dls = mnist.dataloaders(path, bs=256) . learn = vision_learner(dls, resnet18, metrics=accuracy) learn.fit_one_cycle(1, 1e-2) . /home/doyu/mambaforge/lib/python3.9/site-packages/torchvision/models/_utils.py:135: UserWarning: Using &#39;weights&#39; as positional parameter(s) is deprecated since 0.13 and will be removed in 0.15. Please use keyword parameter(s) instead. warnings.warn( /home/doyu/mambaforge/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for &#39;weights&#39; are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights. warnings.warn(msg) . epoch train_loss valid_loss time . 0 | 0.134366 | 0.039583 | 00:13 | . learn = vision_learner(dls, resnet18, metrics=accuracy) learn.fine_tune(1, 1e-2) . epoch train_loss valid_loss accuracy time . 0 | 0.300083 | 0.125544 | 0.955839 | 00:13 | . epoch train_loss valid_loss accuracy time . 0 | 0.039169 | 0.032611 | 0.992149 | 01:27 | .",
            "url": "https://doyu.github.io/blog/2022/08/07/dataloader_from_datablock.html",
            "relUrl": "/2022/08/07/dataloader_from_datablock.html",
            "date": " • Aug 7, 2022"
        }
        
    
  
    
        ,"post14": {
            "title": "DL without Pytorch nn.Module",
            "content": "from fastai.vision.all import * . path = untar_data(URLs.MNIST_SAMPLE) Path.BASE_PATH = path path.ls() . (#3) [Path(&#39;train&#39;),Path(&#39;labels.csv&#39;),Path(&#39;valid&#39;)] . def get_dls(): def get_ds(train_valid): def get_X(train_valid, three_seven): files = (path/train_valid/three_seven).ls() X = torch.stack([tensor(Image.open(x)) for x in files])#.reshape(-1, 28*28) return X X3, X7 = get_X(train_valid, &#39;3&#39;), get_X(train_valid, &#39;7&#39;) X = torch.cat([X3, X7])/255. y = torch.tensor([1]*len(X3) + [0]*len(X7)).reshape(-1, 1) #print(X.shape, y.shape) return list(zip(X, y)) ds1 = DataLoader(get_ds(&#39;train&#39;), bs=256, shuffle=True) ds2 = DataLoader(get_ds(&#39;valid&#39;), bs=256) return DataLoaders(ds1, ds2) dls = get_dls() . def loss_fn(y, t): return torch.where(t==1, 1-y, y).mean() def batch_accuracy(y, t): return ((y&gt;0.5)==t).float().mean() def validate_epoch(model): accs = [batch_accuracy(model(xb.reshape(-1, 28*28)), yb) for xb,yb in dls.valid] return round(torch.stack(accs).mean().item(), 4) class BasicOptim: def __init__(self,params, lr): self.params = list(params) self.lr = lr def step(self, *args, **kwargs): for p in self.params: p.data -= p.grad.data * self.lr def zero_grad(self, *args, **kwargs): for p in self.params: p.grad = None . W = torch.randn(28*28, 1).requires_grad_() b = torch.randn(1).requires_grad_() params = W, b def model(X): return sigmoid(X@W + b) opt = BasicOptim(params, lr=0.1) acc = [] for epoch in range(40): for X, t in dls.train: y = model(X.reshape(-1, 28*28)) loss = loss_fn(y, t) loss.backward() opt.step() opt.zero_grad() acc.append(validate_epoch(model)) . import pandas as pd pd.DataFrame({&#39;accuracy&#39;:acc}).plot(grid=True) . &lt;AxesSubplot:&gt; .",
            "url": "https://doyu.github.io/blog/2022/08/06/pytorch_without_nn.html",
            "relUrl": "/2022/08/06/pytorch_without_nn.html",
            "date": " • Aug 6, 2022"
        }
        
    
  
    
        ,"post15": {
            "title": "DL with Pytorch nn.Module",
            "content": "from fastai.vision.all import * . path = untar_data(URLs.MNIST_SAMPLE) Path.BASE_PATH = path path.ls() . (#3) [Path(&#39;train&#39;),Path(&#39;labels.csv&#39;),Path(&#39;valid&#39;)] . def get_dls(): def get_ds(train_valid): def get_X(train_valid, three_seven): files = (path/train_valid/three_seven).ls() X = torch.stack([tensor(Image.open(x)) for x in files])#.reshape(-1, 28*28) return X X3, X7 = get_X(train_valid, &#39;3&#39;), get_X(train_valid, &#39;7&#39;) X = torch.cat([X3, X7])/255. y = torch.tensor([1]*len(X3) + [0]*len(X7)).reshape(-1, 1) #print(X.shape, y.shape) return list(zip(X, y)) ds1 = DataLoader(get_ds(&#39;train&#39;), bs=256, shuffle=True) ds2 = DataLoader(get_ds(&#39;valid&#39;), bs=256) return DataLoaders(ds1, ds2) dls = get_dls() . def loss_fn(y, t): return torch.where(t==1, 1-y, y).mean() def batch_accuracy(y, t): return ((y&gt;0.5)==t).float().mean() def validate_epoch(model): accs = [batch_accuracy(model(xb), yb) for xb,yb in dls.valid] return round(torch.stack(accs).mean().item(), 4) class BasicOptim: def __init__(self,params, lr): self.params = list(params) self.lr = lr def step(self, *args, **kwargs): for p in self.params: p.data -= p.grad.data * self.lr def zero_grad(self, *args, **kwargs): for p in self.params: p.grad = None opt = BasicOptim(model.parameters(), lr=0.1) . model = nn.Sequential( nn.Flatten(), nn.Linear(28*28,30), nn.ReLU(), nn.Linear(30,1), nn.Sigmoid(), ) acc = [] for epoch in range(40): for X, t in dls.train: y = model(X) loss = loss_fn(y, t) loss.backward() opt.step() opt.zero_grad() acc.append(validate_epoch(model)) . import pandas as pd pd.DataFrame({&#39;accuracy&#39;:acc}).plot(grid=True) . &lt;AxesSubplot:&gt; . if False: learn = Learner(dls, model, opt_func=SGD, loss_func=loss_fn, metrics=batch_accuracy) learn.fit(40, 0.1) plt.plot(L(learn.recorder.values).itemgot(2)); .",
            "url": "https://doyu.github.io/blog/2022/08/06/pytorch_with_nn.html",
            "relUrl": "/2022/08/06/pytorch_with_nn.html",
            "date": " • Aug 6, 2022"
        }
        
    
  
    
        ,"post16": {
            "title": "DL with fastai vision_learner",
            "content": "from fastai.vision.all import * . path = untar_data(URLs.MNIST_SAMPLE) Path.BASE_PATH = path path.ls() . (#3) [Path(&#39;train&#39;),Path(&#39;labels.csv&#39;),Path(&#39;valid&#39;)] . dls = ImageDataLoaders.from_folder(path) learn = vision_learner(dls, resnet18, pretrained=False, loss_func=nn.CrossEntropyLoss(), metrics=accuracy) learn.fit_one_cycle(1, 0.1) . epoch train_loss valid_loss accuracy time . 0 | 0.168187 | 0.021135 | 0.995093 | 00:17 | .",
            "url": "https://doyu.github.io/blog/2022/08/06/fastai_vision_learner.html",
            "relUrl": "/2022/08/06/fastai_vision_learner.html",
            "date": " • Aug 6, 2022"
        }
        
    
  
    
        ,"post17": {
            "title": "DL with fastai Learner",
            "content": "from fastai.vision.all import * . path = untar_data(URLs.MNIST_SAMPLE) Path.BASE_PATH = path path.ls() . (#3) [Path(&#39;train&#39;),Path(&#39;labels.csv&#39;),Path(&#39;valid&#39;)] . def get_dls(): def get_ds(train_valid): def get_X(train_valid, three_seven): files = (path/train_valid/three_seven).ls() X = torch.stack([tensor(Image.open(x)) for x in files])#.reshape(-1, 28*28) return X X3, X7 = get_X(train_valid, &#39;3&#39;), get_X(train_valid, &#39;7&#39;) X = torch.cat([X3, X7])/255. y = torch.tensor([1]*len(X3) + [0]*len(X7)).reshape(-1, 1) #print(X.shape, y.shape) return list(zip(X, y)) ds1 = DataLoader(get_ds(&#39;train&#39;), bs=256, shuffle=True) ds2 = DataLoader(get_ds(&#39;valid&#39;), bs=256) return DataLoaders(ds1, ds2) dls = get_dls() . def loss_fn(y, t): return torch.where(t==1, 1-y, y).mean() model = nn.Sequential( nn.Flatten(), nn.Linear(28*28,30), nn.ReLU(), nn.Linear(30,1), nn.Sigmoid(), ) . def batch_accuracy(y, t): return ((y&gt;0.5)==t).float().mean() . learn = Learner(dls, model, opt_func=SGD, loss_func=loss_fn, metrics=batch_accuracy) learn.fit(40, 0.1) plt.plot(L(learn.recorder.values).itemgot(2)); . epoch train_loss valid_loss batch_accuracy time . 0 | 0.251622 | 0.099996 | 0.966143 | 00:00 | . 1 | 0.117085 | 0.055608 | 0.969087 | 00:00 | . 2 | 0.070977 | 0.044978 | 0.971050 | 00:00 | . 3 | 0.049911 | 0.039824 | 0.971541 | 00:00 | . 4 | 0.039623 | 0.036744 | 0.972031 | 00:00 | . 5 | 0.034272 | 0.034183 | 0.973994 | 00:00 | . 6 | 0.030532 | 0.032801 | 0.974975 | 00:00 | . 7 | 0.028858 | 0.031304 | 0.975466 | 00:00 | . 8 | 0.027127 | 0.030324 | 0.976938 | 00:00 | . 9 | 0.025220 | 0.029530 | 0.977920 | 00:00 | . 10 | 0.024591 | 0.028499 | 0.977920 | 00:00 | . 11 | 0.023528 | 0.027751 | 0.977920 | 00:00 | . 12 | 0.022990 | 0.027213 | 0.977920 | 00:00 | . 13 | 0.022170 | 0.026516 | 0.978901 | 00:00 | . 14 | 0.021096 | 0.026030 | 0.979392 | 00:00 | . 15 | 0.020740 | 0.025526 | 0.979392 | 00:00 | . 16 | 0.020556 | 0.025365 | 0.978901 | 00:00 | . 17 | 0.020185 | 0.024765 | 0.979882 | 00:00 | . 18 | 0.019934 | 0.024219 | 0.979392 | 00:00 | . 19 | 0.019707 | 0.024067 | 0.979392 | 00:00 | . 20 | 0.019217 | 0.023683 | 0.979392 | 00:00 | . 21 | 0.018532 | 0.023350 | 0.979392 | 00:00 | . 22 | 0.018405 | 0.023191 | 0.979882 | 00:00 | . 23 | 0.018165 | 0.022752 | 0.980373 | 00:00 | . 24 | 0.017803 | 0.022621 | 0.980864 | 00:00 | . 25 | 0.017138 | 0.022359 | 0.980864 | 00:00 | . 26 | 0.017201 | 0.022179 | 0.980864 | 00:00 | . 27 | 0.017088 | 0.022125 | 0.981354 | 00:00 | . 28 | 0.016755 | 0.021805 | 0.981845 | 00:00 | . 29 | 0.016481 | 0.021685 | 0.981845 | 00:00 | . 30 | 0.016408 | 0.021350 | 0.981845 | 00:00 | . 31 | 0.016280 | 0.021244 | 0.982336 | 00:00 | . 32 | 0.016051 | 0.020892 | 0.982336 | 00:00 | . 33 | 0.015875 | 0.020904 | 0.982826 | 00:00 | . 34 | 0.015870 | 0.020447 | 0.982336 | 00:00 | . 35 | 0.015795 | 0.020434 | 0.982336 | 00:00 | . 36 | 0.015723 | 0.020223 | 0.981845 | 00:00 | . 37 | 0.015477 | 0.020262 | 0.982826 | 00:00 | . 38 | 0.015137 | 0.020047 | 0.982826 | 00:00 | . 39 | 0.015087 | 0.019813 | 0.982826 | 00:00 | .",
            "url": "https://doyu.github.io/blog/2022/08/06/fastai_learner.html",
            "relUrl": "/2022/08/06/fastai_learner.html",
            "date": " • Aug 6, 2022"
        }
        
    
  
    
        ,"post18": {
            "title": "Should 'y' be 2 dimensional in DataLoader()?",
            "content": "Works OK with y.reshape(-1,1) . from fastai.vision.all import * . path = untar_data(URLs.MNIST_SAMPLE) Path.BASE_PATH = path path.ls() . (#3) [Path(&#39;train&#39;),Path(&#39;labels.csv&#39;),Path(&#39;valid&#39;)] . def dl(p, shuffle=False, reshape_y=True): def f(x): return [tensor(Image.open(o)) for o in (x).ls()] X3 = torch.stack(f(path/p/&#39;3&#39;)) X7 = torch.stack(f(path/p/&#39;7&#39;)) y = tensor([1]*len(X3) + [0]*len(X7)) y = y.reshape(-1, 1) if reshape_y==True else y X = torch.cat([X3, X7]).view(-1, 28*28)/255. ds = list(zip(X, y)) dl = DataLoader(ds, batch_size=256, shuffle=shuffle) return dl def dls(shuffle=True, reshape_y=True): return DataLoaders(dl(&#39;train&#39;, shuffle=shuffle, reshape_y=reshape_y), dl(&#39;valid&#39;, reshape_y=reshape_y)) . def batch_accuracy(xb, yb): preds = xb.sigmoid() correct = (preds&gt;0.5) == yb return correct.float().mean() def mnist_loss(predictions, targets): predictions = predictions.sigmoid() return torch.where(targets==1, 1-predictions, predictions).mean() simple_net = nn.Sequential( nn.Linear(28*28,30), nn.ReLU(), nn.Linear(30,1) ) . learn = Learner(dls(shuffle=True, reshape_y=True), simple_net, opt_func=SGD, loss_func=mnist_loss, metrics=batch_accuracy) learn.fit(5, 0.1) #plt.plot(L(learn.recorder.values).itemgot(0)) #plt.plot(L(learn.recorder.values).itemgot(1)) plt.plot(L(learn.recorder.values).itemgot(2)) . epoch train_loss valid_loss batch_accuracy time . 0 | 0.213742 | 0.085770 | 0.966634 | 00:00 | . 1 | 0.102948 | 0.053242 | 0.969087 | 00:00 | . 2 | 0.064131 | 0.044062 | 0.971050 | 00:00 | . 3 | 0.047448 | 0.039272 | 0.971050 | 00:00 | . 4 | 0.038335 | 0.036503 | 0.972522 | 00:00 | . [&lt;matplotlib.lines.Line2D at 0x7f8384dc5d60&gt;] . Doesn&#39;t work without y.reshape(-1,1) . learn = Learner(dls(shuffle=True, reshape_y=False), simple_net, opt_func=SGD, loss_func=mnist_loss, metrics=batch_accuracy) learn.fit(9, 0.1) #plt.plot(L(learn.recorder.values).itemgot(0)) #plt.plot(L(learn.recorder.values).itemgot(1)) plt.plot(L(learn.recorder.values).itemgot(2)) . epoch train_loss valid_loss batch_accuracy time . 0 | 0.498168 | 0.048625 | 0.959964 | 00:00 | . 1 | 0.497918 | 0.048710 | 0.960454 | 00:00 | . 2 | 0.498148 | 0.048844 | 0.959964 | 00:00 | . 3 | 0.497676 | 0.049057 | 0.958982 | 00:00 | . 4 | 0.497721 | 0.049326 | 0.958492 | 00:00 | . 5 | 0.497653 | 0.049653 | 0.958001 | 00:00 | . 6 | 0.498098 | 0.049928 | 0.958545 | 00:00 | . 7 | 0.498131 | 0.050342 | 0.958108 | 00:00 | . 8 | 0.497700 | 0.050720 | 0.957618 | 00:00 | . [&lt;matplotlib.lines.Line2D at 0x7f8384ff3220&gt;] .",
            "url": "https://doyu.github.io/blog/2022/08/05/dataloader_y_shape.html",
            "relUrl": "/2022/08/05/dataloader_y_shape.html",
            "date": " • Aug 5, 2022"
        }
        
    
  
    
        ,"post19": {
            "title": "Why 'shuffle' prevents trainng?",
            "content": "from fastai.vision.all import * . path = untar_data(URLs.MNIST_SAMPLE) Path.BASE_PATH = path path.ls() . (#3) [Path(&#39;train&#39;),Path(&#39;labels.csv&#39;),Path(&#39;valid&#39;)] . def dl(p, shuffle=False): def f(x): return [tensor(Image.open(o)) for o in (x).ls()] X3 = torch.stack(f(path/p/&#39;3&#39;)) X7 = torch.stack(f(path/p/&#39;7&#39;)) y = tensor([1]*len(X3) + [0]*len(X7)) X = torch.cat([X3, X7]).view(-1, 28*28)/255. ds = list(zip(X, y)) dl = DataLoader(ds, batch_size=256, shuffle=shuffle) return dl . def batch_accuracy(xb, yb): preds = xb.sigmoid() correct = (preds&gt;0.5) == yb return correct.float().mean() def mnist_loss(predictions, targets): predictions = predictions.sigmoid() return torch.where(targets==1, 1-predictions, predictions).mean() . DataLoaders without shuffle in training . This seems to be working OK as seen below. . dls = DataLoaders(dl(&#39;train&#39;), dl(&#39;valid&#39;)) . simple_net = nn.Sequential( nn.Linear(28*28,30), nn.ReLU(), nn.Linear(30,1) ) learn = Learner(dls, simple_net, opt_func=SGD, loss_func=mnist_loss, metrics=batch_accuracy) learn.fit(5, 0.1) plt.plot(L(learn.recorder.values).itemgot(0)) plt.plot(L(learn.recorder.values).itemgot(1)) plt.plot(L(learn.recorder.values).itemgot(2)) . epoch train_loss valid_loss batch_accuracy time . 0 | 0.337977 | 0.415234 | 0.504416 | 00:00 | . 1 | 0.156038 | 0.238818 | 0.793164 | 00:00 | . 2 | 0.085946 | 0.122527 | 0.906396 | 00:00 | . 3 | 0.056482 | 0.086347 | 0.929848 | 00:00 | . 4 | 0.042997 | 0.070332 | 0.946102 | 00:00 | . [&lt;matplotlib.lines.Line2D at 0x7f46b7a02d00&gt;] . DataLoaders with shuffle in training . Both losses{train, valid} and accuracy doesn&#39;t seem to be converging at all, compared with the above dls without shuffle. . dls = DataLoaders(dl(&#39;train&#39;, shuffle=True), dl(&#39;valid&#39;)) . simple_net = nn.Sequential( nn.Linear(28*28,30), nn.ReLU(), nn.Linear(30,1) ) learn = Learner(dls, simple_net, opt_func=SGD, loss_func=mnist_loss, metrics=batch_accuracy) learn.fit(5, 0.1) plt.plot(L(learn.recorder.values).itemgot(0)) plt.plot(L(learn.recorder.values).itemgot(1)) plt.plot(L(learn.recorder.values).itemgot(2)) . epoch train_loss valid_loss batch_accuracy time . 0 | 0.499775 | 0.493191 | 0.509269 | 00:00 | . 1 | 0.499598 | 0.492406 | 0.504416 | 00:00 | . 2 | 0.499982 | 0.491580 | 0.504416 | 00:00 | . 3 | 0.499273 | 0.491103 | 0.504416 | 00:00 | . 4 | 0.499037 | 0.490632 | 0.504416 | 00:01 | . [&lt;matplotlib.lines.Line2D at 0x7f46b6d69df0&gt;] .",
            "url": "https://doyu.github.io/blog/2022/08/04/dl_shuffle.html",
            "relUrl": "/2022/08/04/dl_shuffle.html",
            "date": " • Aug 4, 2022"
        }
        
    
  
    
        ,"post20": {
            "title": "Simple Pytorch SGD",
            "content": "import torch import matplotlib.pyplot as plt . params = torch.tensor([-1.,-2.,-3.]) params . tensor([-1., -2., -3.]) . x = torch.arange(-5,5,0.2) y = 2 * x**2 - 1 * x + 3 + torch.randn_like(x) * 5 . def f(param): a, b, c = param y = a * x**2 + b * x + c return y . plt.scatter(x, y) plt.scatter(x, f(params).detach().cpu().numpy(), color=&#39;red&#39;) . &lt;matplotlib.collections.PathCollection at 0x7faf55fcb1f0&gt; . params.requires_grad_() . tensor([-1., -2., -3.], requires_grad=True) . origs = params.clone() origs . tensor([-1., -2., -3.], grad_fn=&lt;CloneBackward0&gt;) . def mse(): return ((y-f(params))**2).mean() . loss = mse() loss . tensor(1542.2339, grad_fn=&lt;MeanBackward0&gt;) . loss.backward() params.grad . tensor([-863.2866, -3.5042, -63.3594]) . lr = 0.0005 def apply_step(): loss = mse() loss.backward() params.data -= lr * params.grad.data params.grad = None print(loss.item()) . params = origs.detach().requires_grad_() origs = params.clone() for _ in range(9): apply_step() . 1542.23388671875 1191.165771484375 922.902099609375 717.9000244140625 561.22900390625 441.48260498046875 349.94647216796875 279.96307373046875 226.4463348388672 . origs . tensor([-1., -2., -3.], grad_fn=&lt;CloneBackward0&gt;) . params = origs.detach().requires_grad_() _, axs = plt.subplots(3,3, figsize=(24, 24)) for i in range(9): pos = (i//3, i%3) axs[pos].scatter(x, y) axs[pos].scatter(x, f(params).detach().cpu().numpy(), color=&#39;red&#39;) apply_step() . 1542.23388671875 1191.165771484375 922.902099609375 717.9000244140625 561.22900390625 441.48260498046875 349.94647216796875 279.96307373046875 226.4463348388672 .",
            "url": "https://doyu.github.io/blog/2022/08/03/Simple-Pytorch-SGD.html",
            "relUrl": "/2022/08/03/Simple-Pytorch-SGD.html",
            "date": " • Aug 3, 2022"
        }
        
    
  
    
        ,"post21": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://doyu.github.io/blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post22": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://doyu.github.io/blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://doyu.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://doyu.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}